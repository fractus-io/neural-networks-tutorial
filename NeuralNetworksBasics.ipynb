{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "[Neural Networks](#NeuralNetworks)\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\t* [History](#History)\n",
    "\t* [Perceptrons](#Datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <a id=\"NeuralNetworks\"></a>Neural Networks\n",
    "\n",
    "## <a id=\"Introduction\"></a>Introduction\n",
    "\n",
    "### <a id=\"BiologicalNeurons\"></a>Biological Neurons\n",
    "\n",
    "Many tasks that involve intelligence, pattern recognition, object classifications or detection are difficult\n",
    "to implement using classical software engineering principles, even those tasks can easily be performed by animals or young children. \n",
    "\n",
    "For example, family cat can easily recognize you, versus a stranger?\n",
    "Small child can easily recognize who is dad, and who is mom.\n",
    "\n",
    "Human brains can perform complex pattern recognition tasks without even noticing ?\n",
    "How brans can do that ?\n",
    "\n",
    "The answer lies in our bodies. Each of us contains a real-life biological neural\n",
    "networks that is connected to our nervous systems. Network is composed of a large number of\n",
    "interconnected neurons (nerve cells).\n",
    "\n",
    "One brain has approximately 10 billion neurons, each connected to about 10,000\n",
    "other neurons. The cell body of the neuron is called the ***soma***, where the inputs (dendrites) and\n",
    "outputs (axons) connect soma to other soma.\n",
    "\n",
    "Each neuron receives electrochemical inputs from other neurons at their ***dendrites***. If these\n",
    "electrical inputs are powerful enough activate the neuron, then the activated neuron transmits\n",
    "the signal along its ***axon***, passing it along to the ***dendrites*** of other neurons. These attached neurons\n",
    "may also fire, thus continuing the process of passing the message along.\n",
    "\n",
    "Firing a neuron  is a binary operation – the neuron either\n",
    "fires or it doesn’t fire. There are no different ***grades*** of firing. Neuron will fire only \n",
    "if the total signal received at the ***soma*** exceeds a given threshold.\n",
    "\n",
    "![Image](course/assets/image/biological-neurons.png)\n",
    "\n",
    "***Dendrite***: Receives signals from other neurons   \n",
    "***Soma***: Processes the information   \n",
    "***Axon***: Transmits the output of this neuron   \n",
    "***Synapse***: Point of connection to other neurons   \n",
    "\n",
    "Can we simulate neural network from nature ?\n",
    "\n",
    "So, if we simulate brain structure then we should try to implement computation system composed of the connected nodes, \n",
    "where on each node we will execute a simple computation. Such a structure can be implemented using directed graph.\n",
    "From graph theory, we know that a directed graph consists of a set of nodes (i.e., vertices's) and a set of \n",
    "connections (i.e., edges) that are link together.\n",
    "\n",
    "Each node performs a simple computation. Each connection then carries a signal (i.e., the\n",
    "output of the computation) from one node to another, labeled by a weight indicating the extent to\n",
    "which the signal is amplified or diminished. Some connections have large, positive weights that\n",
    "amplify the signal, indicating that the signal is very important when making a classification. \n",
    "Others have negative weights, diminishing the strength of the signal, thus specifying that the output of\n",
    "the node is less important in the final classification. \n",
    "\n",
    "Initially connection weights are defined with random values, which are modified using learning algorithm.\n",
    "\n",
    "Such a system is Artificial Neural Network.\n",
    "\n",
    "The word ***neural*** is the adjective form of ***neuron***, and ***network*** denotes a graph-like\n",
    "structure, therefore, an ***Artificial Neural Network*** is a computation system that attempts to simulate the neural \n",
    "connections in our nervous system. \n",
    "\n",
    "Artificial neural networks are also referred to as ***neural networks***. It is common to abbreviate\n",
    "Artificial Neural Network and refer to them as ***ANN*** or simply ***NN**.\n",
    "\n",
    "### <a id=\"ArtificialNeurons\"></a>Artificial Neurons\n",
    "\n",
    "In 1943 ***Warren S. McCulloch***, a neuroscientist, and ***Walter Pitts, a logician***, published a paper ***A logical calculus of the ideas immanent in nervous activity***. In this paper McCulloch and Pitts tried to understand how the brain could produce highly complex patterns by using many basic cells that are connected together. These basic brain cells are called neurons, and McCulloch and Pitts gave a highly simplified model of a neuron in their paper. \n",
    "\n",
    "The McCulloch and Pitts model of a neuron, which we will call an **MCP neuron** for short, has made an important contribution to the development of artificial neural networks -- which model key features of biological neurons.\n",
    "\n",
    "Model is divided into 2 parts. The first part, ***g*** takes an input, performs an aggregation and based on the aggregated value the second part, ***f*** makes a decision.\n",
    "\n",
    "![Image](course/assets/image/McCullochPittsNeuron.png)\n",
    "\n",
    "The original ***MCP neuron** had limitation, so the the next major development in neural networks was the concept of a ***perceptron*** which was introduced by ***Frank Rosenblatt*** in 1958. Further refined and carefully analyzed by **Minsky** and ***Papert*** (1969) — their model is referred to as the ***perceptron*** model.\n",
    "\n",
    "Essentially the ***perceptron*** is an ***MCP neuron*** where the inputs are first passed through some ***preprocessors*** which are called association units. These association units detect the presence of certain specific features in the inputs. In fact, as the name suggests, a perceptron was intended to be a pattern recognition device, and the association units correspond to feature or pattern detectors.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/perceptron-model.png)\n",
    "\n",
    "\n",
    "The perceptron model, proposed by Minsky-Papert, is a more general computational model than ***MCP neuron***. It overcomes some of the limitations of the ***MCP neuron*** by introducing the concept of numerical weights (a measure of importance) for inputs, and a mechanism for learning those weights. Inputs are no longer limited to boolean values like in the case of an ***MCP neuron***, it supports real inputs as well which makes it more useful and generalized.\n",
    "\n",
    "It takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, number_of_inputs, learning_rate=0.1):\n",
    "        # initialize the weight matrix\n",
    "        np.random.seed(7)\n",
    "        self.W = np.random.randn(number_of_inputs + 1) / np.sqrt(number_of_inputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # activation function    \n",
    "    def step(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "        \n",
    "    def fit(self, X, y, epochs=10 ):\n",
    "        \n",
    "        # add bias to input value\n",
    "        X = np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "        # let's train over desired number of values\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point\n",
    "            for (x, target) in zip(X, y):\n",
    "                \n",
    "                # take the dot product of the input features\n",
    "                # and the weight matrix, then pass calculated value\n",
    "                # through the step function \n",
    "                prediction = self.step(np.dot(x, self.W))\n",
    "                \n",
    "                # update weights if prediction is not same as expected target\n",
    "                if prediction != target:\n",
    "                    # calculate error\n",
    "                    error = prediction - target\n",
    "                    \n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.learning_rate * error * x\n",
    "                    # print(self.W)\n",
    "                    \n",
    "    def predict(self, X, addBias=True):\n",
    "        # ensure our input is a matrix\n",
    "        X = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "            X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product of the input features\n",
    "        # and the weight matrix, then pass calculated value\n",
    "        # through the step function\n",
    "        return self.step(np.dot(X, self.W))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0\n",
      "[INFO] data=[0 1], ground-truth=0, pred=0\n",
      "[INFO] data=[1 0], ground-truth=0, pred=0\n",
      "[INFO] data=[1 1], ground-truth=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "# define AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=1\n",
      "[INFO] data=[1 1], ground-truth=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "# define OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=1\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0\n"
     ]
    }
   ],
   "source": [
    "# define XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
