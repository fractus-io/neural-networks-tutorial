{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "[Neural Networks](#NeuralNetworks)\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\t* [History](#History)\n",
    "\t* [Perceptrons](#Datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <a id=\"NeuralNetworks\"></a>Neural Networks\n",
    "\n",
    "## <a id=\"Introduction\"></a>Introduction\n",
    "\n",
    "### <a id=\"BiologicalNeurons\"></a>Biological Neurons\n",
    "\n",
    "Many tasks that involve intelligence, pattern recognition, object classifications or detection are difficult\n",
    "to implement using classical software engineering principles, even those tasks can easily be performed by animals or young children. \n",
    "\n",
    "For example, family cat can easily recognize you, versus a stranger?\n",
    "Small child can easily recognize who is dad, and who is mom.\n",
    "\n",
    "Human brains can perform complex pattern recognition tasks without even noticing ?\n",
    "How brans can do that ?\n",
    "\n",
    "The answer lies in our bodies. Each of us contains a real-life biological neural\n",
    "networks that is connected to our nervous systems. Network is composed of a large number of\n",
    "interconnected neurons (nerve cells).\n",
    "\n",
    "One brain has approximately 10 billion neurons, each connected to about 10,000\n",
    "other neurons. The cell body of the neuron is called the ***soma***, where the inputs (dendrites) and\n",
    "outputs (axons) connect soma to other soma.\n",
    "\n",
    "Each neuron receives electrochemical inputs from other neurons at their ***dendrites***. If these\n",
    "electrical inputs are powerful enough activate the neuron, then the activated neuron transmits\n",
    "the signal along its ***axon***, passing it along to the ***dendrites*** of other neurons. These attached neurons\n",
    "may also fire, thus continuing the process of passing the message along.\n",
    "\n",
    "Firing a neuron  is a binary operation – the neuron either\n",
    "fires or it doesn’t fire. There are no different ***grades*** of firing. Neuron will fire only \n",
    "if the total signal received at the ***soma*** exceeds a given threshold.\n",
    "\n",
    "![Image](course/assets/image/biological-neurons.png)\n",
    "\n",
    "***Dendrite***: Receives signals from other neurons   \n",
    "***Soma***: Processes the information   \n",
    "***Axon***: Transmits the output of this neuron   \n",
    "***Synapse***: Point of connection to other neurons   \n",
    "\n",
    "Can we simulate neural network from nature ?\n",
    "\n",
    "So, if we simulate brain structure then we should try to implement computation system composed of the connected nodes, \n",
    "where on each node we will execute a simple computation. Such a structure can be implemented using directed graph.\n",
    "From graph theory, we know that a directed graph consists of a set of nodes (i.e., vertices's) and a set of \n",
    "connections (i.e., edges) that are link together.\n",
    "\n",
    "Each node performs a simple computation. Each connection then carries a signal (i.e., the\n",
    "output of the computation) from one node to another, labeled by a weight indicating the extent to\n",
    "which the signal is amplified or diminished. Some connections have large, positive weights that\n",
    "amplify the signal, indicating that the signal is very important when making a classification. \n",
    "Others have negative weights, diminishing the strength of the signal, thus specifying that the output of\n",
    "the node is less important in the final classification. \n",
    "\n",
    "Initially connection weights are defined with random values, which are modified using learning algorithm.\n",
    "\n",
    "Such a system is Artificial Neural Network.\n",
    "\n",
    "The word ***neural*** is the adjective form of ***neuron***, and ***network*** denotes a graph-like\n",
    "structure, therefore, an ***Artificial Neural Network*** is a computation system that attempts to simulate the neural \n",
    "connections in our nervous system. \n",
    "\n",
    "Artificial neural networks are also referred to as ***neural networks***. It is common to abbreviate\n",
    "Artificial Neural Network and refer to them as ***ANN*** or simply ***NN**.\n",
    "\n",
    "### <a id=\"ArtificialNeurons\"></a>Artificial Neurons\n",
    "\n",
    "In 1943 ***Warren S. McCulloch***, a neuroscientist, and ***Walter Pitts, a logician***, published a paper ***A logical calculus of the ideas immanent in nervous activity***. In this paper McCulloch and Pitts tried to understand how the brain could produce highly complex patterns by using many basic cells that are connected together. These basic brain cells are called neurons, and McCulloch and Pitts gave a highly simplified model of a neuron in their paper. \n",
    "\n",
    "The McCulloch and Pitts model of a neuron, which we will call an **MCP neuron** for short, has made an important contribution to the development of artificial neural networks -- which model key features of biological neurons.\n",
    "\n",
    "Model is divided into 2 parts. The first part, ***g*** takes an input, performs an aggregation and based on the aggregated value the second part, ***f*** makes a decision.\n",
    "\n",
    "![Image](course/assets/image/McCullochPittsNeuron.png)\n",
    "\n",
    "The original ***MCP neuron** had limitation, so the the next major development in neural networks was the concept of a ***perceptron*** which was introduced by ***Frank Rosenblatt*** in 1958. Further refined and carefully analyzed by **Minsky** and ***Papert*** (1969) — their model is referred to as the ***perceptron*** model.\n",
    "\n",
    "Essentially the ***perceptron*** is an ***MCP neuron*** where the inputs are first passed through some ***preprocessors*** which are called association units. These association units detect the presence of certain specific features in the inputs. In fact, as the name suggests, a perceptron was intended to be a pattern recognition device, and the association units correspond to feature or pattern detectors.\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "![Image](course/assets/image/perceptron-model.png)\n",
    "\n",
    "\n",
    "The perceptron model, proposed by Minsky-Papert, is a more general computational model than ***MCP neuron***. It overcomes some of the limitations of the ***MCP neuron*** by introducing the concept of numerical weights (a measure of importance) for inputs, and a mechanism for learning those weights. Inputs are no longer limited to boolean values like in the case of an ***MCP neuron***, it supports real inputs as well which makes it more useful and generalized.\n",
    "\n",
    "It takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. \n",
    "\n",
    "Perceptron contains N input nodes, one for each entry in the input row, followed by only one layer in the network with just a single node in that layer.\n",
    "\n",
    "Each input has corresponding weights w1,w2, ... wi from the input. Node takes the weighted sum of inputs and applies a step\n",
    "function(activation function) to determine the output class label. The Perceptron outputs either a 0 or a 1:\n",
    "* 0 for class 1   \n",
    "* 1 for class 2   \n",
    "\n",
    "thus, in its original form, the Perceptron is simply a binary, two class classifier.\n",
    "\n",
    "#### Perceptron Training Procedure and the Delta Rule\n",
    "\n",
    "Training a Perceptron is a fairly straightforward operation. \n",
    "\n",
    "***Goal of the training procedure is to find a set of weights w that correctly classifies each instance in our training set.***\n",
    "\n",
    "\n",
    "In order to trainr Perceptron, we will iteratively feed the network with our training data multiple times. Each time the network has seen the full set of training data, we say an epoch has passed. It normally takes many epochs until a\n",
    "weight vector w can be learned to linearly separate our two classes of data.\n",
    "\n",
    "The pseudocode for the Perceptron training algorithm can be found below:\n",
    "\n",
    "The actual “learning” takes place in Steps 2b and 2c. First, we pass the feature vector xj through\n",
    "the network, take the dot product with the weights w and obtain the output yj. \n",
    "\n",
    "This value is then passed through the step function which will return 1 if x > 0 and 0 otherwise.\n",
    "\n",
    "Now we need to update our weight vector w to step in the direction that is “closer” to the\n",
    "correct classification. \n",
    "\n",
    "This update of the weight vector is handled by the delta rule in Step 2c.\n",
    "\n",
    "The expression (dj􀀀yj) determines if the output classification is correct or not. If the classification\n",
    "is correct, then this difference will be zero. Otherwise, the difference will be either positive or\n",
    "negative, giving us the direction in which our weights will be updated (ultimately bringing us closer\n",
    "to the correct classification). We then multiply (dj 􀀀yj) by xj, moving us closer to the correct\n",
    "classification.\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1. Initialize weights vector w with small random values\n",
    "2. Until Perceptron converges:\n",
    " (a) Loop over each feature vector xj and true class label di in our training set D   \n",
    " (b) Take x and pass it through the network, calculating the output value: yj = f (w(t) \u0001xj)   \n",
    " (c) Update the weights w: wi(t +1) = wi(t)+a(dj 􀀀yj)xj;i for all features 0 <= i <= n   \n",
    "\n",
    "\n",
    "The value delta is our learning rate and controls how large (or small) of a step we take. It’s\n",
    "critical that this value is set correctly. A larger value of a will cause us to take a step in the\n",
    "right direction; however, this step could be too large, and we could easily overstep a local/global\n",
    "optimum.\n",
    "Conversely, a small value of a allows us to take tiny baby steps in the right direction, ensuring\n",
    "we don’t overstep a local/global minimum; however, these tiny baby steps may take an intractable\n",
    "amount of time for our learning to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we understand algorithm behind Perceptrons, let's first implement Perceptron class in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, number_of_inputs, learning_rate=0.1):\n",
    "        # initialize the weight matrix\n",
    "        np.random.seed(7)\n",
    "        self.W = np.random.randn(number_of_inputs + 1) / np.sqrt(number_of_inputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # activation function    \n",
    "    def step(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "        \n",
    "    def fit(self, X, y, epochs=10 ):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each input data\n",
    "            for (x, target) in zip(X, y):\n",
    "                \n",
    "                # calculate dot product of the input features and the weight matrix, \n",
    "                # then pass calculated value through the step function \n",
    "                prediction = self.step(np.dot(x, self.W))\n",
    "                \n",
    "                # update weights, if prediction is not same as expected target value\n",
    "                if prediction != target:\n",
    "                    # calculate error\n",
    "                    error = prediction - target\n",
    "                    \n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.learning_rate * error * x\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product of the input features\n",
    "        # and the weight matrix, then pass calculated value\n",
    "        # through the step function\n",
    "        return self.step(np.dot(X, self.W))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Perceptron Algorithm \n",
    "\n",
    "Let's test Perceptron against AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[0 1], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[1 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[1 1], target-value=1, predicted-value=1\n"
     ]
    }
   ],
   "source": [
    "# define AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "       \n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    \n",
    "    # make a prediction, note: x must be matrix\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Perceptron is successfully trained to predict AND fnction. Now let's try with OR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[0 1], target-value=1, predicted-value=1\n",
      "[INFO] input-data=[1 0], target-value=1, predicted-value=1\n",
      "[INFO] input-data=[1 1], target-value=1, predicted-value=1\n"
     ]
    }
   ],
   "source": [
    "# define OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is successfully trained to predict OR function as well. Now let's try with XOR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=1\n",
      "[INFO] input-data=[0 1], target-value=1, predicted-value=0\n",
      "[INFO] input-data=[1 0], target-value=1, predicted-value=0\n",
      "[INFO] input-data=[1 1], target-value=0, predicted-value=0\n"
     ]
    }
   ],
   "source": [
    "# define XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Perceptron is not able be trained to predict XOR gate. \n",
    "We can play with different learning rates or different\n",
    "weight initialization schemes or with different number of epochs, but Perceptron will never be able to correctly model the \n",
    "XOR function. \n",
    "\n",
    "Reason, XOR function is non linear problem, so we will need neural  network with is more layers and \n",
    "with nonlinear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        \n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    # sigmoif activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    # derivate of the sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point \n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # non-linear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas `D`; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a `for` loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the non-linear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.learning_rate * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value `p`\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a non-linear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.4993851\n",
      "[INFO] epoch=100, loss=0.4932219\n",
      "[INFO] epoch=200, loss=0.4768772\n",
      "[INFO] epoch=300, loss=0.4373929\n",
      "[INFO] epoch=400, loss=0.3808364\n",
      "[INFO] epoch=500, loss=0.3247498\n",
      "[INFO] epoch=600, loss=0.2598484\n",
      "[INFO] epoch=700, loss=0.1452399\n",
      "[INFO] epoch=800, loss=0.0744832\n",
      "[INFO] epoch=900, loss=0.0444825\n",
      "[INFO] epoch=1000, loss=0.0301734\n",
      "[INFO] epoch=1100, loss=0.0222834\n",
      "[INFO] epoch=1200, loss=0.0174274\n",
      "[INFO] epoch=1300, loss=0.0141909\n",
      "[INFO] epoch=1400, loss=0.0119028\n",
      "[INFO] epoch=1500, loss=0.0102110\n",
      "[INFO] epoch=1600, loss=0.0089155\n",
      "[INFO] epoch=1700, loss=0.0078952\n",
      "[INFO] epoch=1800, loss=0.0070731\n",
      "[INFO] epoch=1900, loss=0.0063979\n",
      "[INFO] epoch=2000, loss=0.0058343\n",
      "[INFO] epoch=2100, loss=0.0053575\n",
      "[INFO] epoch=2200, loss=0.0049493\n",
      "[INFO] epoch=2300, loss=0.0045962\n",
      "[INFO] epoch=2400, loss=0.0042880\n",
      "[INFO] epoch=2500, loss=0.0040168\n",
      "[INFO] epoch=2600, loss=0.0037764\n",
      "[INFO] epoch=2700, loss=0.0035621\n",
      "[INFO] epoch=2800, loss=0.0033698\n",
      "[INFO] epoch=2900, loss=0.0031964\n",
      "[INFO] epoch=3000, loss=0.0030393\n",
      "[INFO] epoch=3100, loss=0.0028963\n",
      "[INFO] epoch=3200, loss=0.0027657\n",
      "[INFO] epoch=3300, loss=0.0026459\n",
      "[INFO] epoch=3400, loss=0.0025357\n",
      "[INFO] epoch=3500, loss=0.0024340\n",
      "[INFO] epoch=3600, loss=0.0023398\n",
      "[INFO] epoch=3700, loss=0.0022525\n",
      "[INFO] epoch=3800, loss=0.0021711\n",
      "[INFO] epoch=3900, loss=0.0020953\n",
      "[INFO] epoch=4000, loss=0.0020244\n",
      "[INFO] epoch=4100, loss=0.0019580\n",
      "[INFO] epoch=4200, loss=0.0018956\n",
      "[INFO] epoch=4300, loss=0.0018370\n",
      "[INFO] epoch=4400, loss=0.0017818\n",
      "[INFO] epoch=4500, loss=0.0017297\n",
      "[INFO] epoch=4600, loss=0.0016804\n",
      "[INFO] epoch=4700, loss=0.0016338\n",
      "[INFO] epoch=4800, loss=0.0015897\n",
      "[INFO] epoch=4900, loss=0.0015477\n",
      "[INFO] epoch=5000, loss=0.0015079\n",
      "[INFO] epoch=5100, loss=0.0014700\n",
      "[INFO] epoch=5200, loss=0.0014339\n",
      "[INFO] epoch=5300, loss=0.0013995\n",
      "[INFO] epoch=5400, loss=0.0013667\n",
      "[INFO] epoch=5500, loss=0.0013353\n",
      "[INFO] epoch=5600, loss=0.0013053\n",
      "[INFO] epoch=5700, loss=0.0012766\n",
      "[INFO] epoch=5800, loss=0.0012490\n",
      "[INFO] epoch=5900, loss=0.0012226\n",
      "[INFO] epoch=6000, loss=0.0011973\n",
      "[INFO] epoch=6100, loss=0.0011729\n",
      "[INFO] epoch=6200, loss=0.0011495\n",
      "[INFO] epoch=6300, loss=0.0011270\n",
      "[INFO] epoch=6400, loss=0.0011054\n",
      "[INFO] epoch=6500, loss=0.0010845\n",
      "[INFO] epoch=6600, loss=0.0010644\n",
      "[INFO] epoch=6700, loss=0.0010450\n",
      "[INFO] epoch=6800, loss=0.0010262\n",
      "[INFO] epoch=6900, loss=0.0010081\n",
      "[INFO] epoch=7000, loss=0.0009907\n",
      "[INFO] epoch=7100, loss=0.0009738\n",
      "[INFO] epoch=7200, loss=0.0009574\n",
      "[INFO] epoch=7300, loss=0.0009416\n",
      "[INFO] epoch=7400, loss=0.0009263\n",
      "[INFO] epoch=7500, loss=0.0009114\n",
      "[INFO] epoch=7600, loss=0.0008970\n",
      "[INFO] epoch=7700, loss=0.0008831\n",
      "[INFO] epoch=7800, loss=0.0008696\n",
      "[INFO] epoch=7900, loss=0.0008564\n",
      "[INFO] epoch=8000, loss=0.0008437\n",
      "[INFO] epoch=8100, loss=0.0008313\n",
      "[INFO] epoch=8200, loss=0.0008192\n",
      "[INFO] epoch=8300, loss=0.0008075\n",
      "[INFO] epoch=8400, loss=0.0007961\n",
      "[INFO] epoch=8500, loss=0.0007851\n",
      "[INFO] epoch=8600, loss=0.0007743\n",
      "[INFO] epoch=8700, loss=0.0007638\n",
      "[INFO] epoch=8800, loss=0.0007536\n",
      "[INFO] epoch=8900, loss=0.0007436\n",
      "[INFO] epoch=9000, loss=0.0007339\n",
      "[INFO] epoch=9100, loss=0.0007244\n",
      "[INFO] epoch=9200, loss=0.0007152\n",
      "[INFO] epoch=9300, loss=0.0007062\n",
      "[INFO] epoch=9400, loss=0.0006974\n",
      "[INFO] epoch=9500, loss=0.0006889\n",
      "[INFO] epoch=9600, loss=0.0006805\n",
      "[INFO] epoch=9700, loss=0.0006723\n",
      "[INFO] epoch=9800, loss=0.0006644\n",
      "[INFO] epoch=9900, loss=0.0006566\n",
      "[INFO] epoch=10000, loss=0.0006489\n",
      "[INFO] epoch=10100, loss=0.0006415\n",
      "[INFO] epoch=10200, loss=0.0006342\n",
      "[INFO] epoch=10300, loss=0.0006271\n",
      "[INFO] epoch=10400, loss=0.0006201\n",
      "[INFO] epoch=10500, loss=0.0006133\n",
      "[INFO] epoch=10600, loss=0.0006066\n",
      "[INFO] epoch=10700, loss=0.0006001\n",
      "[INFO] epoch=10800, loss=0.0005937\n",
      "[INFO] epoch=10900, loss=0.0005874\n",
      "[INFO] epoch=11000, loss=0.0005813\n",
      "[INFO] epoch=11100, loss=0.0005752\n",
      "[INFO] epoch=11200, loss=0.0005693\n",
      "[INFO] epoch=11300, loss=0.0005636\n",
      "[INFO] epoch=11400, loss=0.0005579\n",
      "[INFO] epoch=11500, loss=0.0005523\n",
      "[INFO] epoch=11600, loss=0.0005469\n",
      "[INFO] epoch=11700, loss=0.0005416\n",
      "[INFO] epoch=11800, loss=0.0005363\n",
      "[INFO] epoch=11900, loss=0.0005312\n",
      "[INFO] epoch=12000, loss=0.0005261\n",
      "[INFO] epoch=12100, loss=0.0005212\n",
      "[INFO] epoch=12200, loss=0.0005163\n",
      "[INFO] epoch=12300, loss=0.0005115\n",
      "[INFO] epoch=12400, loss=0.0005068\n",
      "[INFO] epoch=12500, loss=0.0005022\n",
      "[INFO] epoch=12600, loss=0.0004977\n",
      "[INFO] epoch=12700, loss=0.0004932\n",
      "[INFO] epoch=12800, loss=0.0004889\n",
      "[INFO] epoch=12900, loss=0.0004846\n",
      "[INFO] epoch=13000, loss=0.0004804\n",
      "[INFO] epoch=13100, loss=0.0004762\n",
      "[INFO] epoch=13200, loss=0.0004721\n",
      "[INFO] epoch=13300, loss=0.0004681\n",
      "[INFO] epoch=13400, loss=0.0004642\n",
      "[INFO] epoch=13500, loss=0.0004603\n",
      "[INFO] epoch=13600, loss=0.0004565\n",
      "[INFO] epoch=13700, loss=0.0004527\n",
      "[INFO] epoch=13800, loss=0.0004490\n",
      "[INFO] epoch=13900, loss=0.0004454\n",
      "[INFO] epoch=14000, loss=0.0004418\n",
      "[INFO] epoch=14100, loss=0.0004382\n",
      "[INFO] epoch=14200, loss=0.0004348\n",
      "[INFO] epoch=14300, loss=0.0004314\n",
      "[INFO] epoch=14400, loss=0.0004280\n",
      "[INFO] epoch=14500, loss=0.0004247\n",
      "[INFO] epoch=14600, loss=0.0004214\n",
      "[INFO] epoch=14700, loss=0.0004182\n",
      "[INFO] epoch=14800, loss=0.0004150\n",
      "[INFO] epoch=14900, loss=0.0004119\n",
      "[INFO] epoch=15000, loss=0.0004088\n",
      "[INFO] epoch=15100, loss=0.0004058\n",
      "[INFO] epoch=15200, loss=0.0004028\n",
      "[INFO] epoch=15300, loss=0.0003999\n",
      "[INFO] epoch=15400, loss=0.0003970\n",
      "[INFO] epoch=15500, loss=0.0003941\n",
      "[INFO] epoch=15600, loss=0.0003913\n",
      "[INFO] epoch=15700, loss=0.0003885\n",
      "[INFO] epoch=15800, loss=0.0003857\n",
      "[INFO] epoch=15900, loss=0.0003830\n",
      "[INFO] epoch=16000, loss=0.0003804\n",
      "[INFO] epoch=16100, loss=0.0003777\n",
      "[INFO] epoch=16200, loss=0.0003751\n",
      "[INFO] epoch=16300, loss=0.0003726\n",
      "[INFO] epoch=16400, loss=0.0003700\n",
      "[INFO] epoch=16500, loss=0.0003675\n",
      "[INFO] epoch=16600, loss=0.0003651\n",
      "[INFO] epoch=16700, loss=0.0003627\n",
      "[INFO] epoch=16800, loss=0.0003603\n",
      "[INFO] epoch=16900, loss=0.0003579\n",
      "[INFO] epoch=17000, loss=0.0003555\n",
      "[INFO] epoch=17100, loss=0.0003532\n",
      "[INFO] epoch=17200, loss=0.0003510\n",
      "[INFO] epoch=17300, loss=0.0003487\n",
      "[INFO] epoch=17400, loss=0.0003465\n",
      "[INFO] epoch=17500, loss=0.0003443\n",
      "[INFO] epoch=17600, loss=0.0003421\n",
      "[INFO] epoch=17700, loss=0.0003400\n",
      "[INFO] epoch=17800, loss=0.0003379\n",
      "[INFO] epoch=17900, loss=0.0003358\n",
      "[INFO] epoch=18000, loss=0.0003337\n",
      "[INFO] epoch=18100, loss=0.0003317\n",
      "[INFO] epoch=18200, loss=0.0003297\n",
      "[INFO] epoch=18300, loss=0.0003277\n",
      "[INFO] epoch=18400, loss=0.0003257\n",
      "[INFO] epoch=18500, loss=0.0003237\n",
      "[INFO] epoch=18600, loss=0.0003218\n",
      "[INFO] epoch=18700, loss=0.0003199\n",
      "[INFO] epoch=18800, loss=0.0003180\n",
      "[INFO] epoch=18900, loss=0.0003162\n",
      "[INFO] epoch=19000, loss=0.0003144\n",
      "[INFO] epoch=19100, loss=0.0003125\n",
      "[INFO] epoch=19200, loss=0.0003107\n",
      "[INFO] epoch=19300, loss=0.0003090\n",
      "[INFO] epoch=19400, loss=0.0003072\n",
      "[INFO] epoch=19500, loss=0.0003055\n",
      "[INFO] epoch=19600, loss=0.0003038\n",
      "[INFO] epoch=19700, loss=0.0003021\n",
      "[INFO] epoch=19800, loss=0.0003004\n",
      "[INFO] epoch=19900, loss=0.0002987\n",
      "[INFO] epoch=20000, loss=0.0002971\n",
      "[INFO] epoch=20100, loss=0.0002955\n",
      "[INFO] epoch=20200, loss=0.0002938\n",
      "[INFO] epoch=20300, loss=0.0002923\n",
      "[INFO] epoch=20400, loss=0.0002907\n",
      "[INFO] epoch=20500, loss=0.0002891\n",
      "[INFO] epoch=20600, loss=0.0002876\n",
      "[INFO] epoch=20700, loss=0.0002861\n",
      "[INFO] epoch=20800, loss=0.0002845\n",
      "[INFO] epoch=20900, loss=0.0002831\n",
      "[INFO] epoch=21000, loss=0.0002816\n",
      "[INFO] epoch=21100, loss=0.0002801\n",
      "[INFO] epoch=21200, loss=0.0002787\n",
      "[INFO] epoch=21300, loss=0.0002772\n",
      "[INFO] epoch=21400, loss=0.0002758\n",
      "[INFO] epoch=21500, loss=0.0002744\n",
      "[INFO] epoch=21600, loss=0.0002730\n",
      "[INFO] epoch=21700, loss=0.0002716\n",
      "[INFO] epoch=21800, loss=0.0002703\n",
      "[INFO] epoch=21900, loss=0.0002689\n",
      "[INFO] epoch=22000, loss=0.0002676\n",
      "[INFO] epoch=22100, loss=0.0002663\n",
      "[INFO] epoch=22200, loss=0.0002649\n",
      "[INFO] epoch=22300, loss=0.0002636\n",
      "[INFO] epoch=22400, loss=0.0002624\n",
      "[INFO] epoch=22500, loss=0.0002611\n",
      "[INFO] epoch=22600, loss=0.0002598\n",
      "[INFO] epoch=22700, loss=0.0002586\n",
      "[INFO] epoch=22800, loss=0.0002573\n",
      "[INFO] epoch=22900, loss=0.0002561\n",
      "[INFO] epoch=23000, loss=0.0002549\n",
      "[INFO] epoch=23100, loss=0.0002537\n",
      "[INFO] epoch=23200, loss=0.0002525\n",
      "[INFO] epoch=23300, loss=0.0002513\n",
      "[INFO] epoch=23400, loss=0.0002501\n",
      "[INFO] epoch=23500, loss=0.0002490\n",
      "[INFO] epoch=23600, loss=0.0002478\n",
      "[INFO] epoch=23700, loss=0.0002467\n",
      "[INFO] epoch=23800, loss=0.0002455\n",
      "[INFO] epoch=23900, loss=0.0002444\n",
      "[INFO] epoch=24000, loss=0.0002433\n",
      "[INFO] epoch=24100, loss=0.0002422\n",
      "[INFO] epoch=24200, loss=0.0002411\n",
      "[INFO] epoch=24300, loss=0.0002400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=24400, loss=0.0002390\n",
      "[INFO] epoch=24500, loss=0.0002379\n",
      "[INFO] epoch=24600, loss=0.0002369\n",
      "[INFO] epoch=24700, loss=0.0002358\n",
      "[INFO] epoch=24800, loss=0.0002348\n",
      "[INFO] epoch=24900, loss=0.0002338\n",
      "[INFO] epoch=25000, loss=0.0002327\n",
      "[INFO] epoch=25100, loss=0.0002317\n",
      "[INFO] epoch=25200, loss=0.0002307\n",
      "[INFO] epoch=25300, loss=0.0002297\n",
      "[INFO] epoch=25400, loss=0.0002288\n",
      "[INFO] epoch=25500, loss=0.0002278\n",
      "[INFO] epoch=25600, loss=0.0002268\n",
      "[INFO] epoch=25700, loss=0.0002259\n",
      "[INFO] epoch=25800, loss=0.0002249\n",
      "[INFO] epoch=25900, loss=0.0002240\n",
      "[INFO] epoch=26000, loss=0.0002230\n",
      "[INFO] epoch=26100, loss=0.0002221\n",
      "[INFO] epoch=26200, loss=0.0002212\n",
      "[INFO] epoch=26300, loss=0.0002203\n",
      "[INFO] epoch=26400, loss=0.0002194\n",
      "[INFO] epoch=26500, loss=0.0002185\n",
      "[INFO] epoch=26600, loss=0.0002176\n",
      "[INFO] epoch=26700, loss=0.0002167\n",
      "[INFO] epoch=26800, loss=0.0002158\n",
      "[INFO] epoch=26900, loss=0.0002149\n",
      "[INFO] epoch=27000, loss=0.0002141\n",
      "[INFO] epoch=27100, loss=0.0002132\n",
      "[INFO] epoch=27200, loss=0.0002124\n",
      "[INFO] epoch=27300, loss=0.0002115\n",
      "[INFO] epoch=27400, loss=0.0002107\n",
      "[INFO] epoch=27500, loss=0.0002099\n",
      "[INFO] epoch=27600, loss=0.0002090\n",
      "[INFO] epoch=27700, loss=0.0002082\n",
      "[INFO] epoch=27800, loss=0.0002074\n",
      "[INFO] epoch=27900, loss=0.0002066\n",
      "[INFO] epoch=28000, loss=0.0002058\n",
      "[INFO] epoch=28100, loss=0.0002050\n",
      "[INFO] epoch=28200, loss=0.0002042\n",
      "[INFO] epoch=28300, loss=0.0002034\n",
      "[INFO] epoch=28400, loss=0.0002027\n",
      "[INFO] epoch=28500, loss=0.0002019\n",
      "[INFO] epoch=28600, loss=0.0002011\n",
      "[INFO] epoch=28700, loss=0.0002004\n",
      "[INFO] epoch=28800, loss=0.0001996\n",
      "[INFO] epoch=28900, loss=0.0001989\n",
      "[INFO] epoch=29000, loss=0.0001981\n",
      "[INFO] epoch=29100, loss=0.0001974\n",
      "[INFO] epoch=29200, loss=0.0001967\n",
      "[INFO] epoch=29300, loss=0.0001959\n",
      "[INFO] epoch=29400, loss=0.0001952\n",
      "[INFO] epoch=29500, loss=0.0001945\n",
      "[INFO] epoch=29600, loss=0.0001938\n",
      "[INFO] epoch=29700, loss=0.0001931\n",
      "[INFO] epoch=29800, loss=0.0001924\n",
      "[INFO] epoch=29900, loss=0.0001917\n",
      "[INFO] epoch=30000, loss=0.0001910\n",
      "[INFO] epoch=30100, loss=0.0001903\n",
      "[INFO] epoch=30200, loss=0.0001896\n",
      "[INFO] epoch=30300, loss=0.0001890\n",
      "[INFO] epoch=30400, loss=0.0001883\n",
      "[INFO] epoch=30500, loss=0.0001876\n",
      "[INFO] epoch=30600, loss=0.0001870\n",
      "[INFO] epoch=30700, loss=0.0001863\n",
      "[INFO] epoch=30800, loss=0.0001857\n",
      "[INFO] epoch=30900, loss=0.0001850\n",
      "[INFO] epoch=31000, loss=0.0001844\n",
      "[INFO] epoch=31100, loss=0.0001837\n",
      "[INFO] epoch=31200, loss=0.0001831\n",
      "[INFO] epoch=31300, loss=0.0001825\n",
      "[INFO] epoch=31400, loss=0.0001818\n",
      "[INFO] epoch=31500, loss=0.0001812\n",
      "[INFO] epoch=31600, loss=0.0001806\n",
      "[INFO] epoch=31700, loss=0.0001800\n",
      "[INFO] epoch=31800, loss=0.0001794\n",
      "[INFO] epoch=31900, loss=0.0001788\n",
      "[INFO] epoch=32000, loss=0.0001782\n",
      "[INFO] epoch=32100, loss=0.0001776\n",
      "[INFO] epoch=32200, loss=0.0001770\n",
      "[INFO] epoch=32300, loss=0.0001764\n",
      "[INFO] epoch=32400, loss=0.0001758\n",
      "[INFO] epoch=32500, loss=0.0001752\n",
      "[INFO] epoch=32600, loss=0.0001746\n",
      "[INFO] epoch=32700, loss=0.0001741\n",
      "[INFO] epoch=32800, loss=0.0001735\n",
      "[INFO] epoch=32900, loss=0.0001729\n",
      "[INFO] epoch=33000, loss=0.0001724\n",
      "[INFO] epoch=33100, loss=0.0001718\n",
      "[INFO] epoch=33200, loss=0.0001712\n",
      "[INFO] epoch=33300, loss=0.0001707\n",
      "[INFO] epoch=33400, loss=0.0001701\n",
      "[INFO] epoch=33500, loss=0.0001696\n",
      "[INFO] epoch=33600, loss=0.0001690\n",
      "[INFO] epoch=33700, loss=0.0001685\n",
      "[INFO] epoch=33800, loss=0.0001680\n",
      "[INFO] epoch=33900, loss=0.0001674\n",
      "[INFO] epoch=34000, loss=0.0001669\n",
      "[INFO] epoch=34100, loss=0.0001664\n",
      "[INFO] epoch=34200, loss=0.0001659\n",
      "[INFO] epoch=34300, loss=0.0001653\n",
      "[INFO] epoch=34400, loss=0.0001648\n",
      "[INFO] epoch=34500, loss=0.0001643\n",
      "[INFO] epoch=34600, loss=0.0001638\n",
      "[INFO] epoch=34700, loss=0.0001633\n",
      "[INFO] epoch=34800, loss=0.0001628\n",
      "[INFO] epoch=34900, loss=0.0001623\n",
      "[INFO] epoch=35000, loss=0.0001618\n",
      "[INFO] epoch=35100, loss=0.0001613\n",
      "[INFO] epoch=35200, loss=0.0001608\n",
      "[INFO] epoch=35300, loss=0.0001603\n",
      "[INFO] epoch=35400, loss=0.0001598\n",
      "[INFO] epoch=35500, loss=0.0001593\n",
      "[INFO] epoch=35600, loss=0.0001589\n",
      "[INFO] epoch=35700, loss=0.0001584\n",
      "[INFO] epoch=35800, loss=0.0001579\n",
      "[INFO] epoch=35900, loss=0.0001574\n",
      "[INFO] epoch=36000, loss=0.0001570\n",
      "[INFO] epoch=36100, loss=0.0001565\n",
      "[INFO] epoch=36200, loss=0.0001560\n",
      "[INFO] epoch=36300, loss=0.0001556\n",
      "[INFO] epoch=36400, loss=0.0001551\n",
      "[INFO] epoch=36500, loss=0.0001547\n",
      "[INFO] epoch=36600, loss=0.0001542\n",
      "[INFO] epoch=36700, loss=0.0001538\n",
      "[INFO] epoch=36800, loss=0.0001533\n",
      "[INFO] epoch=36900, loss=0.0001529\n",
      "[INFO] epoch=37000, loss=0.0001524\n",
      "[INFO] epoch=37100, loss=0.0001520\n",
      "[INFO] epoch=37200, loss=0.0001515\n",
      "[INFO] epoch=37300, loss=0.0001511\n",
      "[INFO] epoch=37400, loss=0.0001507\n",
      "[INFO] epoch=37500, loss=0.0001502\n",
      "[INFO] epoch=37600, loss=0.0001498\n",
      "[INFO] epoch=37700, loss=0.0001494\n",
      "[INFO] epoch=37800, loss=0.0001490\n",
      "[INFO] epoch=37900, loss=0.0001485\n",
      "[INFO] epoch=38000, loss=0.0001481\n",
      "[INFO] epoch=38100, loss=0.0001477\n",
      "[INFO] epoch=38200, loss=0.0001473\n",
      "[INFO] epoch=38300, loss=0.0001469\n",
      "[INFO] epoch=38400, loss=0.0001465\n",
      "[INFO] epoch=38500, loss=0.0001461\n",
      "[INFO] epoch=38600, loss=0.0001457\n",
      "[INFO] epoch=38700, loss=0.0001453\n",
      "[INFO] epoch=38800, loss=0.0001448\n",
      "[INFO] epoch=38900, loss=0.0001445\n",
      "[INFO] epoch=39000, loss=0.0001441\n",
      "[INFO] epoch=39100, loss=0.0001437\n",
      "[INFO] epoch=39200, loss=0.0001433\n",
      "[INFO] epoch=39300, loss=0.0001429\n",
      "[INFO] epoch=39400, loss=0.0001425\n",
      "[INFO] epoch=39500, loss=0.0001421\n",
      "[INFO] epoch=39600, loss=0.0001417\n",
      "[INFO] epoch=39700, loss=0.0001413\n",
      "[INFO] epoch=39800, loss=0.0001410\n",
      "[INFO] epoch=39900, loss=0.0001406\n",
      "[INFO] epoch=40000, loss=0.0001402\n",
      "[INFO] data=[0 0], expected=[0], pred=[[0.00816274]], step=0\n",
      "[INFO] data=[0 1], expected=[1], pred=[[0.9923435]], step=1\n",
      "[INFO] data=[1 0], expected=[1], pred=[[0.99241094]], step=1\n",
      "[INFO] data=[1 1], expected=[0], pred=[[0.00987715]], step=0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork([2, 2, 1], learning_rate=0.5)\n",
    "nn.fit(X, y, epochs=40000)\n",
    "\n",
    "for i in range(0, 4):\n",
    "    x = X[i]\n",
    "    pred = nn.predict(x)\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, expected={}, pred={}, step={}\".format(x, y[i], pred, step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
