{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "[Neural Networks](#NeuralNetworks)\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\t* [History](#History)\n",
    "\t* [Perceptrons](#Datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <a id=\"NeuralNetworks\"></a>Neural Networks\n",
    "\n",
    "## <a id=\"Introduction\"></a>Introduction\n",
    "\n",
    "Many tasks that involves intelligence, pattern recognition, object classifications or detection are difficult\n",
    "to implement using classical software engineering approaches, despite the fact those tasks can easily be performed by animals or children. \n",
    "\n",
    "For example, family cat can easily recognize you, versus a stranger?\n",
    "Small child can easily recognize who is dad, and who is mom.\n",
    "\n",
    "Human brain can perform complex pattern recognition tasks without even noticing, so how brain can do that ?\n",
    "\n",
    "\n",
    "### <a id=\"BiologicalNeurons\"></a>Biological Neurons\n",
    "\n",
    "\n",
    "The answer lies in internal structure of the brain, which is biological neural networks, connected to our nervous systems. Neural Network is composed of a large number of interconnected neurons (nerve cells).\n",
    "\n",
    "One brain has approximately 10 billion neurons, each connected to about 10,000\n",
    "other neurons. The cell body of the neuron is called the ***soma***, where the inputs (dendrites) and\n",
    "outputs (axons) connects soma to other soma.\n",
    "\n",
    "Each neuron receives electrochemical inputs from other neurons at their ***dendrites***. If these\n",
    "electrical inputs are powerful enough to activate neuron, then the activated neuron transmits\n",
    "the signal along its ***axon***, passing it along to the ***dendrites*** of other neurons. These attached neurons\n",
    "may also fire, thus continuing the process of passing the message along.\n",
    "\n",
    "Firing a neuron  is a binary operation, the neuron either\n",
    "fires or it doesn’t fire. There are no different ***grades*** of firing. Neuron will fire only \n",
    "if the total signal received at the ***soma*** exceeds a given threshold.\n",
    "\n",
    "![Image](course/assets/image/biological-neurons.png)\n",
    "\n",
    "***Dendrite***: Receives signals from other neurons   \n",
    "***Soma***: Processes the information   \n",
    "***Axon***: Transmits the output of this neuron   \n",
    "***Synapse***: Point of connection to other neurons   \n",
    "\n",
    "So, is it possible to simulate neural network from nature ?\n",
    "\n",
    "If we simulate brain structure then we should implement computation system composed of the connected nodes, \n",
    "where on each node simple computation will be executed. Such a structure can be implemented using graph structure, which consist of the set of nodes (i.e., vertices's) and a set of connections (i.e., edges) that are link together.\n",
    "\n",
    "Each node performs a simple computation. Each connection carries a signal (i.e., the\n",
    "output of the computation) from one node to another, labeled by a weight indicating the extent to\n",
    "which the signal is amplified or diminished. Some connections have large, positive weights that\n",
    "amplify the signal, indicating that the signal is very important when making a classification. \n",
    "Others have negative weights, diminishing the strength of the signal, thus specifying that the output of\n",
    "the node is less important in the final classification. \n",
    "\n",
    "Initially connection weights are defined with random values, which are modified using learning algorithm.\n",
    "\n",
    "Such a system which implements a capabilities of the biological neural networks, and it is able to perfom tasks such a pattern recognition, object classifications or detection is Artificial Neural Network.\n",
    "\n",
    "\n",
    "### <a id=\"ArtificialNeuralNetwork\"></a>Artificial Neural Network\n",
    "\n",
    "In 1943 ***Warren S. McCulloch***, a neuroscientist, and ***Walter Pitts***, a logician, published a paper ***A logical calculus of the ideas immanent in nervous activity***. In this paper McCulloch and Pitts tried to understand how brain could produce highly complex patterns by using many basic cells that are connected together. These basic brain cells are called neurons, and McCulloch and Pitts gave a highly simplified model of a neuron in their paper. \n",
    "Proposed model is beginning of the ***Artificial Neural Networks***.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/McCullochPittsNeuron.png)\n",
    "\n",
    "\n",
    "After this paper other authors proposed improvements on the ***Artificial Neural Network***, for example a ***Perceptron model*** inveneted by ***Rosenblatt*** at 1958 and then from 1969, ***Multilayer Perceptron (MLP)*** invented by ***Minsky*** and ***Papert***, etc.\n",
    "\n",
    "\n",
    "#### <a id=\"ArtificialNeuralNetwork\"></a>Artificial Neurons\n",
    "\n",
    "The key element of the Artificial Neural Network is Artificial Neuron which simulates Biological Neuron.\n",
    "\n",
    "It contains a nucleus(processing unit), several dendrites(analogous to inputs), and one axon (analogous to output), \n",
    "as shown in the following figure:\n",
    "\n",
    "\n",
    "As we can see in figure our Artificial Neuron has various inputs(plus bias) and each input has randomly assigned weights.\n",
    "The role of neuron is to multiply inputs with weights, sumarize the result of the multiplication, and then to pass sum through activation function. The result of the Activation Function is 0 or 1, so we can say that Activation Function 'fires' a particular neuron or not.\n",
    "\n",
    "\n",
    "##### <a id=\"ActivationFunction\"></a>Activation Function\n",
    "\n",
    "The activation function is a mathematical “gate” between the input feeding the current neuron and its output going to the next neuron. It can be as simple as a step function that turns the neuron output on and off(depending on a rule or threshold), or it can be advanced function, like non-linear fucntion which can help a network to learn complex patterns.\n",
    "\n",
    "There is a lot of activation functions, and we will cover most common. \n",
    "\n",
    "###### Activation Function Types \n",
    "\n",
    "###### Step Functions\n",
    "\n",
    "A binary step function is a threshold-based activation function. If the input value is above or below a certain threshold, the neuron is activated and sends exactly the same signal to the next layer.\n",
    "\n",
    "The function has form:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def step(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y_step = step(x)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(x, y_step, label='Step', color='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with a step function is that it does not allow multi-value outputs, for example, it cannot support classifying the inputs into one of several categories.\n",
    "\n",
    "\n",
    "###### Linear Functions\n",
    "\n",
    "A linear activation function takes the form:\n",
    "\n",
    "$$ f(x) = wx + b $$\n",
    "\n",
    "Linear function is better than a step function because it allows multiple outputs, not just 1 and 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2912efd4668>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear(x):\n",
    "    w = 0.5\n",
    "    b = 1\n",
    "    return w*x + b\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = linear(x)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y, label='Linear', color='r')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks should be able to compute any function. This concept is called Universal approximation theorem. It turns out that is we have more then one neurons in our network, and if only linear activation functions are used, then the combination of these functions is another linear activation function.\n",
    "\n",
    "Problem is that linear functions cannot approximate any function, so we need another type of the function in order to implement Universal approximation theorem, and that type of function is non-linear activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Non-Linear Activation Functions\n",
    "\n",
    "With non-linear activation functions we are able to model to complex mappings between the network’s inputs and outputs, \n",
    "which are essential for learning and modeling complex data, such as images, video, audio, and data sets which are non-linear or have high dimensionality.\n",
    "\n",
    "So, let's se  what are the most common non-linear activaction functions:\n",
    "\n",
    "###### Sigmoid\n",
    "\n",
    "It is a function which is plotted as ***S*** shaped graph. \n",
    "Sigmoid function has following form:   \n",
    "\n",
    "$$ f(x) = 1 / (1 + exp(-x)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y, label='Sigmoid', color='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason why we use sigmoid function is because it exists between 0 to 1. \n",
    "Therefore, it is especially usefull for models where we have to predict the probability as an output.\n",
    "Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "\n",
    "######  Tanh or hyperbolic tangent \n",
    "\n",
    "tanh is similar to sigmoid(***S*** - shaped), but the range of the tanh function is from (-1 to 1).  \n",
    "tanh function has following form:   \n",
    "\n",
    "$$ f(x) = 2 / (1 + exp(-2*x)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tahn(x):\n",
    "    return (2 / (1 + np.exp(-2*x)))-1\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = tahn(x)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y, label='Tann', color='r')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "The tanh function is mainly used for classification between two classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  ReLU (Rectified Linear Unit) \n",
    "\n",
    "ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero. ReLU activation function has following form:   \n",
    "\n",
    "$$ R(y) = max(0, z) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y, label='ReLU', color='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ReLU is the most used activation function in the world these days, while it is storngly used in convolutional neural networks.\n",
    "\n",
    "\n",
    "\n",
    "So, in this paragraph, we mention most common activation functions, and the decision which function to use depends of the concrete use case where will apply our artificial neuron/network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### <a id=\"InputsWeights\"></a>Inputs Weights(Parameters)\n",
    "\n",
    "As we already mentioned, artificial neuron has various inputs(plus bias) and each input has randomly assigned weights.\n",
    "Weight represent the connections between neurons and have the capability to amplify or attenuate neuron signals, for example, multiply the signals, thus modifying them. \n",
    "\n",
    "So, by modifying the signals, weights have the power to influence a neuron's output, \n",
    "therefore a neuron's activation will be dependent on the inputs and the weights. \n",
    "\n",
    "\n",
    "##### <a id=\"BiasParameter\"></a>Bias Parameter\n",
    "\n",
    "\n",
    "The artificial neuron can have an independent component that adds an extra signal to the activation function. This component is called bias.\n",
    "\n",
    "Just like the inputs, biases also have an associated weight. \n",
    "This feature helps in the neural network knowledge representation as a more purely nonlinear system.\n",
    "\n",
    "It is an extra input to neurons and it is always 1, and has it’s own connection weight. This makes sure that even when all the inputs are none (all 0’s) there’s gonna be an activation in the neuron.\n",
    "\n",
    "\n",
    "#### <a id=\"Layers\"></a>Layers\n",
    "\n",
    "Now we understand what neuron is, and how it works, and if we connect one neuron with another neurons, that will form neural network. Neurons will be connected with concept where output of one neuron is input to another neuron. This approach will give us huge amount of possible ways how to connect neurons or in another words, how to organize neural network. Therefore concept of layers has is used.\n",
    "\n",
    "Neural network is organized in layers where all neurons in a layer share a same input and activation function.\n",
    "Neural networks are composed of several linked layers, forming the so-called multilayer networks. The neural layers are divided into three classes:\n",
    "\n",
    "* ***Input Layer***   \n",
    "     is the first layer in the neural network, it takes input signals(values) and passes them on to the next layer. \n",
    "     It doesn’t apply any operations on the input signals(values) & has no weights and biases values associated. \n",
    "     \n",
    "* ***Hidden Layer***   \n",
    "     hidden layer is a collection of neurons stacked vertically. All the neurons in a hidden layer are connected to each and every neuron in the next layer. Last hidden layer passes on values to the output layer.\n",
    "     \n",
    "* ***Output Layer***   \n",
    "    is the last layer in the network and it receives input from the last hidden layer.\n",
    "    \n",
    "\n",
    "As we can see, neuron can be connected to another neuron forming a neural network, so it is obvious that we can connect neurons in a different layouts. So, the way how neural network is defined is called neural network architecture.\n",
    "\n",
    "\n",
    "#### <a id=\"NeuralNetworkArchitectures\"></a>Neural Network Architectures\n",
    "\n",
    "Architectures for neural networks are defined based on the neuron connections, and signal flow. Therefore we can split neural networks based on neuron connections and signal flow.\n",
    "\n",
    "* ***Neuron connections***\n",
    "\n",
    "    * ***Monolayer networks***   \n",
    "        all neurons are laid out in the same level, forming one single layer, as shown in the figure:\n",
    "\n",
    "    * ***Multilayer networks***   \n",
    "        neurons are divided into multiple layers, each layer corresponding to a parallel layout of neurons that shares the same input data, as shown in the figure:\n",
    "\n",
    "\n",
    "* ***Signal flow***\n",
    "\n",
    "    * **Feeedforward networks***   \n",
    "        flow of the signals goes in one direction or in recurrence, input signals are fed into the input layer, then, after being processed, they are forwarded to the next layer, as it is shown in the figure:\n",
    "\n",
    "    * ***Feedback networks***   \n",
    "        signals are fed back in a neuron or layer that has already received and processed that signal. See the following figure of feedback networks:\n",
    "\n",
    "\n",
    "\n",
    "### <a id=\"NeuralNetworkLearning\"></a>Neural Network Learning\n",
    "\n",
    "Until now we learned what neuron is, how it works, and how we can build neural network by connecting neurons and organizing them in a layers. But how neural network is able to learn ?\n",
    "\n",
    "First let's discuss what learning process is. \n",
    "\n",
    "Humans are able to learn from observations without any support, or we can have a teacher who shows us the right pattern to follow. First aproach is called ***supervised*** and second is called ***unsupervised***. Same approach can be aplied to Artficial NEural Network.\n",
    "\n",
    "#### Supervised learning\n",
    "This category of learning deals in pairs of X's and Y's, and the objective is to map them in a function f: X › Y. Here, the Y data is the supervisor, the target desired outputs, and the X data is the source-independent data that generates the Y data. It is analogous to a teacher who is teaching somebody a certain task to be performed, as shown in the following figure:\n",
    "\n",
    "One particular feature of this learning paradigm is that there is a direct error reference, which is just the comparison between the target and the current actual result. The network parameters are fed into a cost function, which quantifies the mismatch between the desired and the actual outputs.\n",
    "\n",
    "Supervised learning is very suitable for tasks that already provide a pattern, a goal to be reached. Some examples are as follows: classification of images, speech recognition, function approximation, and forecasting. Note that the neural network should be provided previous knowledge of both input-independent values (X) and the output classification-dependent values (Y). The presence of a dependent output value is a necessary condition for the learning to be supervised.\n",
    "\n",
    "\n",
    "#### Unsupervised learning\n",
    "As illustrated in the following figure, in unsupervised learning, we deal only with data without any labeling or classification; instead, our neural structure tries to draw inferences and extract knowledge by taking into account only the input data X.\n",
    "\n",
    "This is analogous to self-learning, when someone learns by him/herself taking into account his/her experience and a set of supporting criteria. In unsupervised learning, we don't have a defined desired pattern to be applied on each observation, but the neural structure can produce one by itself without any supervising need.\n",
    "\n",
    "\n",
    "\n",
    "There are basically two types of learning supervised and unsupervised. Humans learns on a The learning in the human mind, for example, also works in this way. We can learn from observations without any kind of target pattern (unsupervised), or we can have a teacher who shows us the right pattern to follow (supervised). The difference between these two paradigms relies mainly on the relevance of a target pattern and varies from problem to problem.\n",
    "\n",
    "\n",
    "\n",
    "Consider that we have a task where solution solve may have a huge number of theoretically possible solutions, the learning process seeks to find an optimal solution that can produce a satisfying result.\n",
    "\n",
    "The use of structures like artificial neural networks (ANNs) is encouraged because of their ability to acquire knowledge of any type, strictly by receiving input stimuli, that is, data relevant to the task/problem. First, the ANN will produce a random result and an error, and based on this error, the ANN parameters will be adjusted.\n",
    "\n",
    "\n",
    "What is really amazing about neural networks is their capacity to learn from the environment, just like brain-gifted beings are able to. We, as humans, experience the learning process through observations and repetitions, until some task or concept is completely mastered. From the physiological point of view, the learning process in the human brain is a reconfiguration of the neural connections between the nodes (neurons), which results in a new thinking structure.\n",
    "\n",
    "While the connectionist nature of neural networks distributes the learning process all over the entire structure, this feature makes this structure flexible enough to learn a wide variety of knowledge. As opposed to ordinary digital computers that can execute only those tasks that they are programmed to, neural systems are able to improve and perform new activities according to some satisfaction criteria. In other words, neural networks don't need to be programmed; they learn the program by themselves.\n",
    "\n",
    "How learning helps to solve problems\n",
    "\n",
    "Considering that every task that requires solving solve may have a huge number of theoretically possible solutions, the learning process seeks to find an optimal solution that can produce a satisfying result. The use of structures like artificial neural networks (ANNs) is encouraged because of their ability to acquire knowledge of any type, strictly by receiving input stimuli, that is, data relevant to the task/problem. First, the ANN will produce a random result and an error, and based on this error, the ANN parameters will be adjusted.\n",
    "\n",
    "#### Learning paradigms\n",
    "\n",
    "There are basically two types of learning for neural networks, namely supervised and unsupervised. The learning in the human mind, for example, also works in this way. We can learn from observations without any kind of target pattern (unsupervised), or we can have a teacher who shows us the right pattern to follow (supervised). The difference between these two paradigms relies mainly on the relevance of a target pattern and varies from problem to problem.\n",
    "\n",
    "Supervised learning\n",
    "This category of learning deals in pairs of X's and Y's, and the objective is to map them in a function f: X › Y. Here, the Y data is the supervisor, the target desired outputs, and the X data is the source-independent data that generates the Y data. It is analogous to a teacher who is teaching somebody a certain task to be performed, as shown in the following figure:\n",
    "\n",
    "One particular feature of this learning paradigm is that there is a direct error reference, which is just the comparison between the target and the current actual result. The network parameters are fed into a cost function, which quantifies the mismatch between the desired and the actual outputs.\n",
    "\n",
    "Supervised learning is very suitable for tasks that already provide a pattern, a goal to be reached. Some examples are as follows: classification of images, speech recognition, function approximation, and forecasting. Note that the neural network should be provided previous knowledge of both input-independent values (X) and the output classification-dependent values (Y). The presence of a dependent output value is a necessary condition for the learning to be supervised.\n",
    "\n",
    "\n",
    "Unsupervised learning\n",
    "As illustrated in the following figure, in unsupervised learning, we deal only with data without any labeling or classification; instead, our neural structure tries to draw inferences and extract knowledge by taking into account only the input data X.\n",
    "\n",
    "This is analogous to self-learning, when someone learns by him/herself taking into account his/her experience and a set of supporting criteria. In unsupervised learning, we don't have a defined desired pattern to be applied on each observation, but the neural structure can produce one by itself without any supervising need.\n",
    "\n",
    "#### Systematic structuring – learning algorithm\n",
    "\n",
    "So far, we have theoretically defined the learning process and how it is carried out. However, in practice, we must dive a little bit deeper into the mathematical logic, the learning algorithm itself. A learning algorithm is a procedure that drives the learning process of neural networks and is strongly determined by the neural network architecture. From the mathematical point of view, one wishes to find the optimal weights W that can drive the cost function C(X,[Y]) to the lowest possible value.\n",
    "\n",
    "In general, this process is carried out in the fashion presented in the following flowchart:\n",
    "\n",
    "Just like any program that we wish to write, we should have defined our goal, so in here, we are talking about a neural network to learn some knowledge. We should present this knowledge (or environment) to the ANN and check its response, which naturally will make no sense. The network response is then compared to the expected result, and this is fed to a cost function C. This cost function will determine how the weights W can be updated. The learning algorithm then computes the ?W term, which means the variation of the values of the weights to be added. The weights are updated as in the equation.\n",
    "\n",
    "\n",
    "Where k refers to the kth iteration and W(k) refers to the neural weights at the kth iteration, and subsequently, k + 1 refers to the next iteration.\n",
    "\n",
    "As the learning process is run, the neural network must give results closer and closer to the expectation, until finally, it reaches the acceptation criteria. The learning process is then considered to be finished.\n",
    "\n",
    "Two stages of learning – training and testing\n",
    "Well, we might ask now whether the neural network has already learned from the data, but how can we attest it has effectively learnt the data? The answer is just like in the exams that students are subjected to; we need to check the network response after training. But wait! Do you think it is likely that a teacher would put in an exam the same questions he/she has presented in the classes? There is no sense in evaluating somebody's learning with examples that are already known or a suspecting teacher would conclude the student might have memorized the content, instead of having learnt it.\n",
    "\n",
    "Okay, let's now explain this part. What we are talking about here is testing. The learning process that we have covered is called training. After training a neural network, we should test it whether it has really learnt. For testing, we must present to the neural network another fraction of data from the same environment that it has learnt from. This is necessary because, just like the student, the neural network could respond properly with only the data points that it had been exposed to; this is called overtraining. To check whether the neural network has not passed on overtraining, we must check its response to other data points.\n",
    "\n",
    "The following figure illustrates the overtraining problem. Imagine that our network is designed to approximate some function f(x) whose definition is unknown. The neural network was fed with some data from that function and produced the following result shown in the figure on the left. However, when expanding to a wider domain, we note that the neural response does not follow the data.\n",
    "\n",
    "\n",
    "In this case, we see that the neural network failed to learn the whole environment (the function f(x)). This happens because of a number of reasons:\n",
    "\n",
    "The neural network didn't receive enough information from the environment\n",
    "\n",
    "The data from the environment is nondeterministic\n",
    "\n",
    "The training and testing datasets are poorly defined\n",
    "\n",
    "The neural network has learnt a lot from the training data and forgets about the testing data\n",
    "\n",
    "\n",
    "\n",
    "The details – learning parameters\n",
    "The learning process may be, and is recommended to be, controlled. One important parameter is the learning rate, often represented by the Greek letter ?. This parameter dictates how strongly the neural weights would vary in the weights' hyperspace. Let's imagine a simple neural network with two inputs and one neuron, therefore one output. So, we've got two weights w1 and w2. Now suppose that we want to train this network and imagine whether we could evaluate the error for each pair of weights. Suppose that we found a surface like the one in the following figure:\n",
    "\n",
    "\n",
    "The learning rate is responsible for regulating how far the weights are going to move on the surface. This may speed up the learning process but can also lead to a set of weights worse than the previous one.\n",
    "\n",
    "Another important parameter is the condition for stopping. Usually, the training stops when the general mean error is reached, but there are cases in which the network fails to learn and there is little or no change in the weights' values. In the latter case, the maximum number of iterations, or epochs, is the condition for stopping.\n",
    "\n",
    "Error measurement and cost function\n",
    "This is extremely important for the success of the training in the supervised learning. Let's suppose that we present for the network a set of N records containing pairs of X and T variables, whereas X are the input-independent values and T are the target values dependent on X. Let's consider the neural network as a mathematical function ANN() that produces Y on the output when being fed with the X values.\n",
    "\n",
    "\n",
    "For each x value given to the ANN, it will produce a y value that when compared to the t value gives an error e.\n",
    "\n",
    "\n",
    "However, this is a mere individual error measurement per data point. We should take into account a general measurement, covering all the N data pairs because we want the network to learn all the data points and the same weights must be able to produce the data covering the entire training set. That's the role of the cost function C.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Where X are the inputs, T are the target outputs, W are the weights, x[i] is the input at the ith instant, and t[i] is the target output for the ith instant. The result of this function is an overall measurement of the error between the target outputs and the neural outputs, and this should be minimized.\n",
    "\n",
    "\n",
    "\n",
    "***************\n",
    "\n",
    "\n",
    "Learning algorithm, percpetrons\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "![Image](course/assets/image/perceptron-model.png)\n",
    "\n",
    "The original ***MCP neuron** had limitation, so the the next major development in neural networks was the concept of a ***perceptron*** which was introduced by ***Frank Rosenblatt*** in 1958. Further refined and carefully analyzed by **Minsky** and ***Papert*** (1969), their model is referred to as the ***perceptron*** model.\n",
    "\n",
    "Essentially the ***perceptron*** is an ***MCP neuron*** where the inputs are first passed through some ***preprocessors*** which are called association units. These association units detect the presence of certain specific features in the inputs. In fact, as the name suggests, a perceptron was intended to be a pattern recognition device, and the association units correspond to feature or pattern detectors.\n",
    "\n",
    "The perceptron model, proposed by Minsky-Papert, is a more general computational model than ***MCP neuron***. It overcomes some of the limitations of the ***MCP neuron*** by introducing the concept of numerical weights (a measure of importance) for inputs, and a mechanism for learning those weights. Inputs are no longer limited to boolean values like in the case of an ***MCP neuron***, it supports real inputs as well which makes it more useful and generalized.\n",
    "\n",
    "It takes an input, aggregates it(weighted sum) and returns 1 only if the aggregated sum is more than threshold and in all other case returns 0. \n",
    "\n",
    "Perceptron contains N input nodes, one for each entry in the input row, followed by only one layer in the network with just a single node in that layer.\n",
    "\n",
    "Each input has corresponding weights w1,w2, ... wi from the input. Node takes the weighted sum of inputs and applies a step\n",
    "function(activation function) to determine the output class label. The Perceptron outputs either a 0 or a 1:\n",
    "* 0 for class 1   \n",
    "* 1 for class 2   \n",
    "\n",
    "thus, in its original form, the Perceptron is simply a binary, two class classifier.\n",
    "\n",
    "#### Perceptron Training Procedure and the Delta Rule\n",
    "\n",
    "Training a Perceptron is a fairly straightforward operation. \n",
    "\n",
    "***Goal of the training procedure is to find a set of weights w that correctly classifies each instance in our training set.***\n",
    "\n",
    "\n",
    "In order to train Perceptron, we will iteratively feed the network with our training data multiple times. Each time the network has seen the full set of training data, we say an epoch has passed. It normally takes many epochs until a\n",
    "weight vector w can be learned to linearly separate our two classes of data.\n",
    "\n",
    "The pseudocode for the Perceptron training algorithm can be found below:\n",
    "\n",
    "The actual “learning” takes place in Steps 2b and 2c. First, we pass the feature vector xj through\n",
    "the network, take the dot product with the weights w and obtain the output yj. \n",
    "\n",
    "This value is then passed through the step function which will return 1 if x > 0 and 0 otherwise.\n",
    "\n",
    "Now we need to update our weight vector w to step in the direction that is “closer” to the\n",
    "correct classification. \n",
    "\n",
    "This update of the weight vector is handled by the delta rule in Step 2c.\n",
    "\n",
    "The expression (dj􀀀yj) determines if the output classification is correct or not. If the classification\n",
    "is correct, then this difference will be zero. Otherwise, the difference will be either positive or\n",
    "negative, giving us the direction in which our weights will be updated (ultimately bringing us closer\n",
    "to the correct classification). We then multiply (dj 􀀀yj) by xj, moving us closer to the correct\n",
    "classification.\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1. Initialize weights vector w with small random values\n",
    "2. Until Perceptron converges:\n",
    " (a) Loop over each feature vector xj and true class label di in our training set D   \n",
    " (b) Take x and pass it through the network, calculating the output value: yj = f (w(t) \u0001xj)   \n",
    " (c) Update the weights w: wi(t +1) = wi(t)+a(dj 􀀀yj)xj;i for all features 0 <= i <= n   \n",
    "\n",
    "\n",
    "The value delta is our learning rate and controls how large (or small) of a step we take. It’s\n",
    "critical that this value is set correctly. A larger value of a will cause us to take a step in the\n",
    "right direction; however, this step could be too large, and we could easily overstep a local/global\n",
    "optimum.\n",
    "Conversely, a small value of a allows us to take tiny baby steps in the right direction, ensuring\n",
    "we don’t overstep a local/global minimum; however, these tiny baby steps may take an intractable\n",
    "amount of time for our learning to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we understand algorithm behind Perceptrons, let's first implement Perceptron class in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, number_of_inputs, learning_rate=0.1):\n",
    "        # initialize the weight matrix\n",
    "        np.random.seed(7)\n",
    "        self.W = np.random.randn(number_of_inputs + 1) / np.sqrt(number_of_inputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # activation function    \n",
    "    def step(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "        \n",
    "    def fit(self, X, y, epochs=10 ):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each input data\n",
    "            for (x, target) in zip(X, y):\n",
    "                \n",
    "                # calculate dot product of the input features and the weight matrix, \n",
    "                # then pass calculated value through the step function \n",
    "                prediction = self.step(np.dot(x, self.W))\n",
    "                \n",
    "                # update weights, if prediction is not same as expected target value\n",
    "                if prediction != target:\n",
    "                    # calculate error\n",
    "                    error = prediction - target\n",
    "                    \n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.learning_rate * error * x\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product of the input features\n",
    "        # and the weight matrix, then pass calculated value\n",
    "        # through the step function\n",
    "        return self.step(np.dot(X, self.W))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, following functionality has been implemented:\n",
    "\n",
    "* ***init*** functions randomly initializes weights, and keeps basic hyperparameters like ***learning_rate***\n",
    "* ***step*** function implements simple step based activation function   \n",
    "* ***fit*** function is a place where training is done, method loops over the defined number of epochs, calculates sum between weights and input values, applies step function on that sum, and updates the wieghts using delta rule\n",
    "* ***predict*** function calculates the output based on inputs and trained weights\n",
    "\n",
    "Now our algorithm is ready for evaluation.\n",
    "\n",
    "\n",
    "#### Evaluating Perceptron Algorithm \n",
    "\n",
    "Let's test Perceptron against AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "       \n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    \n",
    "    # make a prediction, note: x must be matrix\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Perceptron is successfully trained to predict AND function. Now let's try with OR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is successfully trained to predict OR function as well. Now let's try with XOR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Perceptron is not able be trained to predict XOR gate. \n",
    "We can play with different learning rates or different\n",
    "weight initialization schemes or with different number of epochs, but Perceptron will never be able to correctly model the \n",
    "XOR function. \n",
    "\n",
    "Perceptron algortihm has single layer, therefore it is called Single Layer Perceptron and in fact it is Artificial Neural Network.\n",
    "\n",
    "A single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1 , 0).\n",
    "\n",
    "![Image](course/assets/image/single-layer-perceptron.png)\n",
    "\n",
    "Algorithm\t\t\n",
    "The single layer perceptron does not have a priori knowledge, so the initial weights are assigned randomly. SLP sums all the weighted inputs and if the sum is above the threshold (some predetermined value), SLP is said to be activated (output=1). \t\n",
    "\n",
    "\n",
    "![Image](course/assets/image/slp-algorithm.png)\n",
    "\n",
    "The input values are presented to the perceptron, and if the predicted output is the same as the desired output, then the performance is considered satisfactory and no changes to the weights are made. However, if the output does not match the desired output, then the weights need to be changed to reduce the error. \n",
    "\n",
    "\n",
    "![Image](course/assets/image/slp-weights-update.png)\n",
    "\n",
    "Because SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n",
    "\n",
    "\t\t\n",
    "However, a multi-layer perceptron using the backpropagation algorithm can successfully classify the XOR data.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/xor.png)\n",
    "\n",
    "\n",
    "\n",
    "Multi-layer Perceptron - Backpropagation algorithm\t\t\n",
    "A multi-layer perceptron (MLP) has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values. \t\t\n",
    " \t\t\n",
    "Forward propagation:\t\t\n",
    "Propagate inputs by adding all the weighted inputs and then computing outputs using sigmoid threshold.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/mlp-forward-pass.png)\n",
    "\n",
    "\n",
    "Backward propagation:\t\t\n",
    "Propagates the errors backward by apportioning them to each unit according to the amount of this error the unit is responsible for.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/mlp-backward-pass.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reason, XOR function is non linear problem, so we will need neural  network with more layers and \n",
    "with nonlinear activation functions.\n",
    "First let's dig deeper into the theory behind Artificial Neural Networks.\n",
    "\n",
    "\n",
    "### Artificial Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        \n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    # sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    # derivate of the sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point \n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # non-linear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas `D`; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a `for` loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the non-linear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.learning_rate * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value `p`\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a non-linear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork([2, 2, 1], learning_rate=0.5)\n",
    "nn.fit(X, y, epochs=40000)\n",
    "\n",
    "for i in range(0, 4):\n",
    "    x = X[i]\n",
    "    pred = nn.predict(x)\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, expected={}, pred={}, step={}\".format(x, y[i], pred, step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
