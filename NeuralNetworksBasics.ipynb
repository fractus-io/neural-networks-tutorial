{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "[Neural Networks](#NeuralNetworks)\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\t* [History](#History)\n",
    "\t* [Perceptrons](#Datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <a id=\"NeuralNetworks\"></a>Neural Networks\n",
    "\n",
    "## <a id=\"Introduction\"></a>Introduction\n",
    "\n",
    "Many tasks that involves intelligence, pattern recognition, object classifications or detection are difficult\n",
    "to implement using classical software engineering principles, even those tasks can easily be performed by animals or young children. \n",
    "\n",
    "For example, family cat can easily recognize you, versus a stranger?\n",
    "Small child can easily recognize who is dad, and who is mom.\n",
    "\n",
    "Human brains can perform complex pattern recognition tasks without even noticing ?   \n",
    "How brains can do that ?\n",
    "\n",
    "\n",
    "### <a id=\"BiologicalNeurons\"></a>Biological Neurons\n",
    "\n",
    "\n",
    "The answer lies in our brains, which is biological neural networks, connected to our nervous systems. Neural Network is composed of a large number of interconnected neurons (nerve cells).\n",
    "\n",
    "One brain has approximately 10 billion neurons, each connected to about 10,000\n",
    "other neurons. The cell body of the neuron is called the ***soma***, where the inputs (dendrites) and\n",
    "outputs (axons) connects soma to other soma.\n",
    "\n",
    "Each neuron receives electrochemical inputs from other neurons at their ***dendrites***. If these\n",
    "electrical inputs are powerful enough to activate neuron, then the activated neuron transmits\n",
    "the signal along its ***axon***, passing it along to the ***dendrites*** of other neurons. These attached neurons\n",
    "may also fire, thus continuing the process of passing the message along.\n",
    "\n",
    "Firing a neuron  is a binary operation, the neuron either\n",
    "fires or it doesn’t fire. There are no different ***grades*** of firing. Neuron will fire only \n",
    "if the total signal received at the ***soma*** exceeds a given threshold.\n",
    "\n",
    "![Image](course/assets/image/biological-neurons.png)\n",
    "\n",
    "***Dendrite***: Receives signals from other neurons   \n",
    "***Soma***: Processes the information   \n",
    "***Axon***: Transmits the output of this neuron   \n",
    "***Synapse***: Point of connection to other neurons   \n",
    "\n",
    "So, is it possible to simulate neural network from nature ?\n",
    "\n",
    "If we simulate brain structure then we should implement computation system composed of the connected nodes, \n",
    "where on each node simple computation will be executed. Such a structure can be implemented using graph structure, which consist of the set of nodes (i.e., vertices's) and a set of connections (i.e., edges) that are link together.\n",
    "\n",
    "Each node performs a simple computation. Each connection carries a signal (i.e., the\n",
    "output of the computation) from one node to another, labeled by a weight indicating the extent to\n",
    "which the signal is amplified or diminished. Some connections have large, positive weights that\n",
    "amplify the signal, indicating that the signal is very important when making a classification. \n",
    "Others have negative weights, diminishing the strength of the signal, thus specifying that the output of\n",
    "the node is less important in the final classification. \n",
    "\n",
    "Initially connection weights are defined with random values, which are modified using learning algorithm.\n",
    "\n",
    "Such a system which implements a capabilities of the biological neural networks, and it is able to perfom tasks such a pattern recognition, object classifications or detection is Artificial Neural Network.\n",
    "\n",
    "\n",
    "### <a id=\"ArtificialNeuralNetwork\"></a>Artificial Neural Network\n",
    "\n",
    "In 1943 ***Warren S. McCulloch***, a neuroscientist, and ***Walter Pitts***, a logician, published a paper ***A logical calculus of the ideas immanent in nervous activity***. In this paper McCulloch and Pitts tried to understand how the brain could produce highly complex patterns by using many basic cells that are connected together. These basic brain cells are called neurons, and McCulloch and Pitts gave a highly simplified model of a neuron in their paper. \n",
    "Proposed model is beginning of the ***Artificial Neural Network***.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/McCullochPittsNeuron.png)\n",
    "\n",
    "\n",
    "Later on there was additional improvements of the ***Artificial Neural Network***, for example a Perceptron model inveneted by Rosenblatt at 1958 and then from 1969 Multilayer Perceptron (MLP) invented by Minsky and Papert, etc.\n",
    "\n",
    "\n",
    "#### <a id=\"ArtificialNeuralNetwork\"></a>Artificial Neurons\n",
    "\n",
    "The key element of the Artificial Neural Network is Artificial Neuron which simulates Biological Neuron.\n",
    "\n",
    "It contains a nucleus(processing unit), several dendrites(analogous to inputs), and one axon (analogous to output), \n",
    "as shown in the following figure:\n",
    "\n",
    "\n",
    "As we can see in figure our Artificial Neuron has various inputs(including bias) and each input has randomly assigned weight.\n",
    "The role of neuron is to multiply inputs with weights, sumarize the result of the multiplication, and then to pass sum through activation function. The result of the Activation Function is 0 or 1, so we can say that Activation Function 'fire' a particular neuron or not.\n",
    "\n",
    "\n",
    "#### <a id=\"ActivationFunction\"></a>Activation Function\n",
    "\n",
    "The main purpuse of the activation function is to convert input signal of an neuron in ANN to an output signal. That output signal is used as input for the next layer in the ANN.\n",
    "Main function of the AF is to introduce non-linear properties in network, which helps a model to understand the comlexity and give accurate results. \n",
    "Without AF, ANN will not be able to to learn and model comlex patterns which can be founded in images, videos, audio etc.\n",
    "\n",
    "The AF can be divided in two types, linear and non-linear.\n",
    "\n",
    "\n",
    "\n",
    "In natural neuron, there is a thershold potential that when it is reached, fires the axon ans propagates the signal to other neurons. This firing behavior is simulated with activation function.\n",
    "\n",
    "The artificial neuron's output is processed by an activation function. This component adds nonlinearity to neural network processing, which is needed because the natural neuron has nonlinear behaviors. An activation function is usually bounded between two values at the output, therefore being a nonlinear function, but in some special cases, it can be a linear function.\n",
    "\n",
    "The most used activation functions are as follows:\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "Hyperbolic tangent\n",
    "\n",
    "Step function\n",
    "\n",
    "Purely linear\n",
    "\n",
    "The equations and charts associated with these functions are shown in the following table:\n",
    "....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def step(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y_step = step(x)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "#plt.plot(x, y_step, label='Step', color='k', lw=1, linestyle=None)\n",
    "#plt.plot(x, y_sigmoid, label='Sigmoid', color='k', lw=1, ls='--')\n",
    "#plt.plot(x, y_relu, label='ReLU', color='k', lw=1, linestyle='-.')\n",
    "\n",
    "plt.plot(x, y_step, label='Step', linestyle=None)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid',  ls='--')\n",
    "plt.plot(x, y_relu, label='ReLU', lw=1, linestyle='-.')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### <a id=\"InputsWeights\"></a>Inputs Weights\n",
    "\n",
    "\n",
    "In neural networks, weights represent the connections between neurons and have the capability to amplify or attenuate neuron signals, for example, multiply the signals, thus modifying them. \n",
    "\n",
    "So, by modifying the neural network signals, neural weights have the power to influence a neuron's output, \n",
    "therefore a neuron's activation will be dependent on the inputs and on the weights. \n",
    "\n",
    "Thus, since the weights are internal to the neural network and influence its outputs, we can consider them as neural network knowledge, provided that changing the weights will change the neural network's capabilities and therefore actions.\n",
    "\n",
    "\n",
    "#### <a id=\"BiasParameter\"></a>Bias Parameter\n",
    "\n",
    "\n",
    "The artificial neuron can have an independent component that adds an extra signal to the activation function. This component is called bias.\n",
    "\n",
    "Just like the inputs, biases also have an associated weight. \n",
    "This feature helps in the neural network knowledge representation as a more purely nonlinear system.\n",
    "\n",
    "\n",
    "#### <a id=\"Layers\"></a>Layers\n",
    "\n",
    "Natural neurons are organized in layers, each one providing a specific level of processing; \n",
    "for example, the input layer receives direct stimuli from the outside world, and the output layers fire actions that will have a direct influence on the outside world. Between these layers, there are a number of hidden layers, in the sense that they do not interact directly with the outside world. \n",
    "\n",
    "In the artificial neural networks, all neurons in a layer share the same inputs and activation function, as shown in the following figure:\n",
    "\n",
    "\n",
    "Neural networks can be composed of several linked layers, forming the so-called multilayer networks. The neural layers can be basically divided into three classes:\n",
    "\n",
    "Input layer\n",
    "\n",
    "Hidden layer\n",
    "\n",
    "Output layer\n",
    "\n",
    "\n",
    "#### <a id=\"NeuralNetworkArchitectures\"></a>Neural Network Architectures\n",
    "\n",
    "As we can see,  neuron as a basic building element can be connected to another neuron(or neurons) forming a neural network, so it is obvious that we can connect neuruns in a different layouts. So, the way how neural network is defined is called neural network architecture.\n",
    "\n",
    "Since, neural networks can be applied to a number of problems, and depending on the nature of the problem, the neural network should be designed in order to address this problem more efficiently.\n",
    "\n",
    "Architectures for neural networks are defined based on the neuron connections, and signal flow. Therefore we have following neural networs:\n",
    "\n",
    "* Neuron connections\n",
    "\n",
    "    * Monolayer networks\n",
    "\n",
    "In this architecture, all neurons are laid out in the same level, forming one single layer, as shown in the following figure\n",
    "\n",
    "The neural network receives the input signals and feeds them into the neurons, which in turn produce the output signals. The neurons can be highly connected to each other with or without recurrence. Examples of these architectures are the single-layer perceptron, Adaline, self-organizing map, Elman, and Hopfield neural networks.\n",
    "\n",
    "    * Multilayer networks\n",
    "\n",
    "In this category, neurons are divided into multiple layers, each layer corresponding to a parallel layout of neurons that shares the same input data, as shown in the following figure:\n",
    "\n",
    "Radial basis functions and multilayer perceptrons are good examples of this architecture. Such networks are really useful for approximating real data to a function specially designed to represent that data. Moreover, because they have multiple layers of processing, these networks are adapted to learn from nonlinear data, being able to separate it or determine more easily the knowledge that reproduces or recognizes this data.\n",
    "\n",
    "\n",
    "* Signal flow\n",
    "\n",
    "    * Feedforward networks\n",
    "\n",
    "The flow of the signals in neural networks can be either in only one direction or in recurrence. In the first case, we call the neural network architecture feedforward, since the input signals are fed into the input layer; then, after being processed, they are forwarded to the next layer, just as shown in the figure in the multilayer section. Multilayer perceptrons and radial basis functions are also good examples of feedforward networks.\n",
    "\n",
    "    * Feedback networks\n",
    "\n",
    "When the neural network has some kind of internal recurrence, it means that the signals are fed back in a neuron or layer that has already received and processed that signal, the network is of the type feedback. See the following figure of feedback networks:\n",
    "\n",
    "\n",
    "The special reason to add recurrence in the network is the production of a dynamic behavior, particularly when the network addresses problems involving time series or pattern recognition, that require an internal memory to reinforce the learning process. However, such networks are particularly difficult to train, eventually failing to learn. Most of the feedback networks are single layer, such as Elman and Hopfield networks, but it is possible to build a recurrent multilayer network, such as echo and recurrent multilayer perceptron networks.\n",
    "\n",
    "\n",
    "### <a id=\"NeuralNetworkLearning\"></a>Neural Network Learning\n",
    "\n",
    "What is really amazing about neural networks is their capacity to learn from the environment, just like brain-gifted beings are able to. We, as humans, experience the learning process through observations and repetitions, until some task or concept is completely mastered. From the physiological point of view, the learning process in the human brain is a reconfiguration of the neural connections between the nodes (neurons), which results in a new thinking structure.\n",
    "\n",
    "While the connectionist nature of neural networks distributes the learning process all over the entire structure, this feature makes this structure flexible enough to learn a wide variety of knowledge. As opposed to ordinary digital computers that can execute only those tasks that they are programmed to, neural systems are able to improve and perform new activities according to some satisfaction criteria. In other words, neural networks don't need to be programmed; they learn the program by themselves.\n",
    "\n",
    "How learning helps to solve problems\n",
    "Considering that every task that requires solving solve may have a huge number of theoretically possible solutions, the learning process seeks to find an optimal solution that can produce a satisfying result. The use of structures like artificial neural networks (ANNs) is encouraged because of their ability to acquire knowledge of any type, strictly by receiving input stimuli, that is, data relevant to the task/problem. First, the ANN will produce a random result and an error, and based on this error, the ANN parameters will be adjusted.\n",
    "\n",
    "#### Learning paradigms\n",
    "\n",
    "There are basically two types of learning for neural networks, namely supervised and unsupervised. The learning in the human mind, for example, also works in this way. We can learn from observations without any kind of target pattern (unsupervised), or we can have a teacher who shows us the right pattern to follow (supervised). The difference between these two paradigms relies mainly on the relevance of a target pattern and varies from problem to problem.\n",
    "\n",
    "Supervised learning\n",
    "This category of learning deals in pairs of X's and Y's, and the objective is to map them in a function f: X › Y. Here, the Y data is the supervisor, the target desired outputs, and the X data is the source-independent data that generates the Y data. It is analogous to a teacher who is teaching somebody a certain task to be performed, as shown in the following figure:\n",
    "\n",
    "One particular feature of this learning paradigm is that there is a direct error reference, which is just the comparison between the target and the current actual result. The network parameters are fed into a cost function, which quantifies the mismatch between the desired and the actual outputs.\n",
    "\n",
    "Supervised learning is very suitable for tasks that already provide a pattern, a goal to be reached. Some examples are as follows: classification of images, speech recognition, function approximation, and forecasting. Note that the neural network should be provided previous knowledge of both input-independent values (X) and the output classification-dependent values (Y). The presence of a dependent output value is a necessary condition for the learning to be supervised.\n",
    "\n",
    "\n",
    "Unsupervised learning\n",
    "As illustrated in the following figure, in unsupervised learning, we deal only with data without any labeling or classification; instead, our neural structure tries to draw inferences and extract knowledge by taking into account only the input data X.\n",
    "\n",
    "This is analogous to self-learning, when someone learns by him/herself taking into account his/her experience and a set of supporting criteria. In unsupervised learning, we don't have a defined desired pattern to be applied on each observation, but the neural structure can produce one by itself without any supervising need.\n",
    "\n",
    "#### Systematic structuring – learning algorithm\n",
    "\n",
    "So far, we have theoretically defined the learning process and how it is carried out. However, in practice, we must dive a little bit deeper into the mathematical logic, the learning algorithm itself. A learning algorithm is a procedure that drives the learning process of neural networks and is strongly determined by the neural network architecture. From the mathematical point of view, one wishes to find the optimal weights W that can drive the cost function C(X,[Y]) to the lowest possible value.\n",
    "\n",
    "In general, this process is carried out in the fashion presented in the following flowchart:\n",
    "\n",
    "Just like any program that we wish to write, we should have defined our goal, so in here, we are talking about a neural network to learn some knowledge. We should present this knowledge (or environment) to the ANN and check its response, which naturally will make no sense. The network response is then compared to the expected result, and this is fed to a cost function C. This cost function will determine how the weights W can be updated. The learning algorithm then computes the ?W term, which means the variation of the values of the weights to be added. The weights are updated as in the equation.\n",
    "\n",
    "\n",
    "Where k refers to the kth iteration and W(k) refers to the neural weights at the kth iteration, and subsequently, k + 1 refers to the next iteration.\n",
    "\n",
    "As the learning process is run, the neural network must give results closer and closer to the expectation, until finally, it reaches the acceptation criteria. The learning process is then considered to be finished.\n",
    "\n",
    "Two stages of learning – training and testing\n",
    "Well, we might ask now whether the neural network has already learned from the data, but how can we attest it has effectively learnt the data? The answer is just like in the exams that students are subjected to; we need to check the network response after training. But wait! Do you think it is likely that a teacher would put in an exam the same questions he/she has presented in the classes? There is no sense in evaluating somebody's learning with examples that are already known or a suspecting teacher would conclude the student might have memorized the content, instead of having learnt it.\n",
    "\n",
    "Okay, let's now explain this part. What we are talking about here is testing. The learning process that we have covered is called training. After training a neural network, we should test it whether it has really learnt. For testing, we must present to the neural network another fraction of data from the same environment that it has learnt from. This is necessary because, just like the student, the neural network could respond properly with only the data points that it had been exposed to; this is called overtraining. To check whether the neural network has not passed on overtraining, we must check its response to other data points.\n",
    "\n",
    "The following figure illustrates the overtraining problem. Imagine that our network is designed to approximate some function f(x) whose definition is unknown. The neural network was fed with some data from that function and produced the following result shown in the figure on the left. However, when expanding to a wider domain, we note that the neural response does not follow the data.\n",
    "\n",
    "\n",
    "In this case, we see that the neural network failed to learn the whole environment (the function f(x)). This happens because of a number of reasons:\n",
    "\n",
    "The neural network didn't receive enough information from the environment\n",
    "\n",
    "The data from the environment is nondeterministic\n",
    "\n",
    "The training and testing datasets are poorly defined\n",
    "\n",
    "The neural network has learnt a lot from the training data and forgets about the testing data\n",
    "\n",
    "In this book, we will cover this process to prevent this and other issues that may arise during training.\n",
    "\n",
    "The details – learning parameters\n",
    "The learning process may be, and is recommended to be, controlled. One important parameter is the learning rate, often represented by the Greek letter ?. This parameter dictates how strongly the neural weights would vary in the weights' hyperspace. Let's imagine a simple neural network with two inputs and one neuron, therefore one output. So, we've got two weights w1 and w2. Now suppose that we want to train this network and imagine whether we could evaluate the error for each pair of weights. Suppose that we found a surface like the one in the following figure:\n",
    "\n",
    "\n",
    "The learning rate is responsible for regulating how far the weights are going to move on the surface. This may speed up the learning process but can also lead to a set of weights worse than the previous one.\n",
    "\n",
    "Another important parameter is the condition for stopping. Usually, the training stops when the general mean error is reached, but there are cases in which the network fails to learn and there is little or no change in the weights' values. In the latter case, the maximum number of iterations, or epochs, is the condition for stopping.\n",
    "\n",
    "Error measurement and cost function\n",
    "This is extremely important for the success of the training in the supervised learning. Let's suppose that we present for the network a set of N records containing pairs of X and T variables, whereas X are the input-independent values and T are the target values dependent on X. Let's consider the neural network as a mathematical function ANN() that produces Y on the output when being fed with the X values.\n",
    "\n",
    "\n",
    "For each x value given to the ANN, it will produce a y value that when compared to the t value gives an error e.\n",
    "\n",
    "\n",
    "However, this is a mere individual error measurement per data point. We should take into account a general measurement, covering all the N data pairs because we want the network to learn all the data points and the same weights must be able to produce the data covering the entire training set. That's the role of the cost function C.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Where X are the inputs, T are the target outputs, W are the weights, x[i] is the input at the ith instant, and t[i] is the target output for the ith instant. The result of this function is an overall measurement of the error between the target outputs and the neural outputs, and this should be minimized.\n",
    "\n",
    "\n",
    "\n",
    "***************\n",
    "\n",
    "\n",
    "Learning algorithm, percpetrons\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "![Image](course/assets/image/perceptron-model.png)\n",
    "\n",
    "The original ***MCP neuron** had limitation, so the the next major development in neural networks was the concept of a ***perceptron*** which was introduced by ***Frank Rosenblatt*** in 1958. Further refined and carefully analyzed by **Minsky** and ***Papert*** (1969), their model is referred to as the ***perceptron*** model.\n",
    "\n",
    "Essentially the ***perceptron*** is an ***MCP neuron*** where the inputs are first passed through some ***preprocessors*** which are called association units. These association units detect the presence of certain specific features in the inputs. In fact, as the name suggests, a perceptron was intended to be a pattern recognition device, and the association units correspond to feature or pattern detectors.\n",
    "\n",
    "The perceptron model, proposed by Minsky-Papert, is a more general computational model than ***MCP neuron***. It overcomes some of the limitations of the ***MCP neuron*** by introducing the concept of numerical weights (a measure of importance) for inputs, and a mechanism for learning those weights. Inputs are no longer limited to boolean values like in the case of an ***MCP neuron***, it supports real inputs as well which makes it more useful and generalized.\n",
    "\n",
    "It takes an input, aggregates it(weighted sum) and returns 1 only if the aggregated sum is more than threshold and in all other case returns 0. \n",
    "\n",
    "Perceptron contains N input nodes, one for each entry in the input row, followed by only one layer in the network with just a single node in that layer.\n",
    "\n",
    "Each input has corresponding weights w1,w2, ... wi from the input. Node takes the weighted sum of inputs and applies a step\n",
    "function(activation function) to determine the output class label. The Perceptron outputs either a 0 or a 1:\n",
    "* 0 for class 1   \n",
    "* 1 for class 2   \n",
    "\n",
    "thus, in its original form, the Perceptron is simply a binary, two class classifier.\n",
    "\n",
    "#### Perceptron Training Procedure and the Delta Rule\n",
    "\n",
    "Training a Perceptron is a fairly straightforward operation. \n",
    "\n",
    "***Goal of the training procedure is to find a set of weights w that correctly classifies each instance in our training set.***\n",
    "\n",
    "\n",
    "In order to train Perceptron, we will iteratively feed the network with our training data multiple times. Each time the network has seen the full set of training data, we say an epoch has passed. It normally takes many epochs until a\n",
    "weight vector w can be learned to linearly separate our two classes of data.\n",
    "\n",
    "The pseudocode for the Perceptron training algorithm can be found below:\n",
    "\n",
    "The actual “learning” takes place in Steps 2b and 2c. First, we pass the feature vector xj through\n",
    "the network, take the dot product with the weights w and obtain the output yj. \n",
    "\n",
    "This value is then passed through the step function which will return 1 if x > 0 and 0 otherwise.\n",
    "\n",
    "Now we need to update our weight vector w to step in the direction that is “closer” to the\n",
    "correct classification. \n",
    "\n",
    "This update of the weight vector is handled by the delta rule in Step 2c.\n",
    "\n",
    "The expression (dj􀀀yj) determines if the output classification is correct or not. If the classification\n",
    "is correct, then this difference will be zero. Otherwise, the difference will be either positive or\n",
    "negative, giving us the direction in which our weights will be updated (ultimately bringing us closer\n",
    "to the correct classification). We then multiply (dj 􀀀yj) by xj, moving us closer to the correct\n",
    "classification.\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1. Initialize weights vector w with small random values\n",
    "2. Until Perceptron converges:\n",
    " (a) Loop over each feature vector xj and true class label di in our training set D   \n",
    " (b) Take x and pass it through the network, calculating the output value: yj = f (w(t) \u0001xj)   \n",
    " (c) Update the weights w: wi(t +1) = wi(t)+a(dj 􀀀yj)xj;i for all features 0 <= i <= n   \n",
    "\n",
    "\n",
    "The value delta is our learning rate and controls how large (or small) of a step we take. It’s\n",
    "critical that this value is set correctly. A larger value of a will cause us to take a step in the\n",
    "right direction; however, this step could be too large, and we could easily overstep a local/global\n",
    "optimum.\n",
    "Conversely, a small value of a allows us to take tiny baby steps in the right direction, ensuring\n",
    "we don’t overstep a local/global minimum; however, these tiny baby steps may take an intractable\n",
    "amount of time for our learning to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we understand algorithm behind Perceptrons, let's first implement Perceptron class in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, number_of_inputs, learning_rate=0.1):\n",
    "        # initialize the weight matrix\n",
    "        np.random.seed(7)\n",
    "        self.W = np.random.randn(number_of_inputs + 1) / np.sqrt(number_of_inputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # activation function    \n",
    "    def step(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "        \n",
    "    def fit(self, X, y, epochs=10 ):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each input data\n",
    "            for (x, target) in zip(X, y):\n",
    "                \n",
    "                # calculate dot product of the input features and the weight matrix, \n",
    "                # then pass calculated value through the step function \n",
    "                prediction = self.step(np.dot(x, self.W))\n",
    "                \n",
    "                # update weights, if prediction is not same as expected target value\n",
    "                if prediction != target:\n",
    "                    # calculate error\n",
    "                    error = prediction - target\n",
    "                    \n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.learning_rate * error * x\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product of the input features\n",
    "        # and the weight matrix, then pass calculated value\n",
    "        # through the step function\n",
    "        return self.step(np.dot(X, self.W))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, following functionality has been implemented:\n",
    "\n",
    "* ***init*** functions randomly initializes weights, and keeps basic hyperparameters like ***learning_rate***\n",
    "* ***step*** function implements simple step based activation function   \n",
    "* ***fit*** function is a place where training is done, method loops over the defined number of epochs, calculates sum between weights and input values, applies step function on that sum, and updates the wieghts using delta rule\n",
    "* ***predict*** function calculates the output based on inputs and trained weights\n",
    "\n",
    "Now our algorithm is ready for evaluation.\n",
    "\n",
    "\n",
    "#### Evaluating Perceptron Algorithm \n",
    "\n",
    "Let's test Perceptron against AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[0 1], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[1 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[1 1], target-value=1, predicted-value=1\n"
     ]
    }
   ],
   "source": [
    "# define AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "       \n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    \n",
    "    # make a prediction, note: x must be matrix\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Perceptron is successfully trained to predict AND function. Now let's try with OR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=0\n",
      "[INFO] input-data=[0 1], target-value=1, predicted-value=1\n",
      "[INFO] input-data=[1 0], target-value=1, predicted-value=1\n",
      "[INFO] input-data=[1 1], target-value=1, predicted-value=1\n"
     ]
    }
   ],
   "source": [
    "# define OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is successfully trained to predict OR function as well. Now let's try with XOR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input-data=[0 0], target-value=0, predicted-value=1\n",
      "[INFO] input-data=[0 1], target-value=1, predicted-value=0\n",
      "[INFO] input-data=[1 0], target-value=1, predicted-value=0\n",
      "[INFO] input-data=[1 1], target-value=0, predicted-value=0\n"
     ]
    }
   ],
   "source": [
    "# define XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# obtain perceptron\n",
    "p = Perceptron(X.shape[1], learning_rate=0.1)\n",
    "\n",
    "# train\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = p.predict(np.atleast_2d(x))\n",
    "    print(\"[INFO] input-data={}, target-value={}, predicted-value={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Perceptron is not able be trained to predict XOR gate. \n",
    "We can play with different learning rates or different\n",
    "weight initialization schemes or with different number of epochs, but Perceptron will never be able to correctly model the \n",
    "XOR function. \n",
    "\n",
    "Perceptron algortihm has single layer, therefore it is called Single Layer Perceptron and in fact it is Artificial Neural Network.\n",
    "\n",
    "A single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1 , 0).\n",
    "\n",
    "![Image](course/assets/image/single-layer-perceptron.png)\n",
    "\n",
    "Algorithm\t\t\n",
    "The single layer perceptron does not have a priori knowledge, so the initial weights are assigned randomly. SLP sums all the weighted inputs and if the sum is above the threshold (some predetermined value), SLP is said to be activated (output=1). \t\n",
    "\n",
    "\n",
    "![Image](course/assets/image/slp-algorithm.png)\n",
    "\n",
    "The input values are presented to the perceptron, and if the predicted output is the same as the desired output, then the performance is considered satisfactory and no changes to the weights are made. However, if the output does not match the desired output, then the weights need to be changed to reduce the error. \n",
    "\n",
    "\n",
    "![Image](course/assets/image/slp-weights-update.png)\n",
    "\n",
    "Because SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n",
    "\n",
    "\t\t\n",
    "However, a multi-layer perceptron using the backpropagation algorithm can successfully classify the XOR data.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/xor.png)\n",
    "\n",
    "\n",
    "\n",
    "Multi-layer Perceptron - Backpropagation algorithm\t\t\n",
    "A multi-layer perceptron (MLP) has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values. \t\t\n",
    " \t\t\n",
    "Forward propagation:\t\t\n",
    "Propagate inputs by adding all the weighted inputs and then computing outputs using sigmoid threshold.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/mlp-forward-pass.png)\n",
    "\n",
    "\n",
    "Backward propagation:\t\t\n",
    "Propagates the errors backward by apportioning them to each unit according to the amount of this error the unit is responsible for.\n",
    "\n",
    "\n",
    "![Image](course/assets/image/mlp-backward-pass.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reason, XOR function is non linear problem, so we will need neural  network with more layers and \n",
    "with nonlinear activation functions.\n",
    "First let's dig deeper into the theory behind Artificial Neural Networks.\n",
    "\n",
    "\n",
    "### Artificial Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        \n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    # sigmoif activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    # derivate of the sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        \n",
    "        # insert a column of 1's as the last entry in the feature matrix (bias)\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # start training\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point \n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # non-linear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas `D`; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a `for` loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the non-linear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.learning_rate * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value `p`\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a non-linear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.4993851\n",
      "[INFO] epoch=100, loss=0.4932219\n",
      "[INFO] epoch=200, loss=0.4768772\n",
      "[INFO] epoch=300, loss=0.4373929\n",
      "[INFO] epoch=400, loss=0.3808364\n",
      "[INFO] epoch=500, loss=0.3247498\n",
      "[INFO] epoch=600, loss=0.2598484\n",
      "[INFO] epoch=700, loss=0.1452399\n",
      "[INFO] epoch=800, loss=0.0744832\n",
      "[INFO] epoch=900, loss=0.0444825\n",
      "[INFO] epoch=1000, loss=0.0301734\n",
      "[INFO] epoch=1100, loss=0.0222834\n",
      "[INFO] epoch=1200, loss=0.0174274\n",
      "[INFO] epoch=1300, loss=0.0141909\n",
      "[INFO] epoch=1400, loss=0.0119028\n",
      "[INFO] epoch=1500, loss=0.0102110\n",
      "[INFO] epoch=1600, loss=0.0089155\n",
      "[INFO] epoch=1700, loss=0.0078952\n",
      "[INFO] epoch=1800, loss=0.0070731\n",
      "[INFO] epoch=1900, loss=0.0063979\n",
      "[INFO] epoch=2000, loss=0.0058343\n",
      "[INFO] epoch=2100, loss=0.0053575\n",
      "[INFO] epoch=2200, loss=0.0049493\n",
      "[INFO] epoch=2300, loss=0.0045962\n",
      "[INFO] epoch=2400, loss=0.0042880\n",
      "[INFO] epoch=2500, loss=0.0040168\n",
      "[INFO] epoch=2600, loss=0.0037764\n",
      "[INFO] epoch=2700, loss=0.0035621\n",
      "[INFO] epoch=2800, loss=0.0033698\n",
      "[INFO] epoch=2900, loss=0.0031964\n",
      "[INFO] epoch=3000, loss=0.0030393\n",
      "[INFO] epoch=3100, loss=0.0028963\n",
      "[INFO] epoch=3200, loss=0.0027657\n",
      "[INFO] epoch=3300, loss=0.0026459\n",
      "[INFO] epoch=3400, loss=0.0025357\n",
      "[INFO] epoch=3500, loss=0.0024340\n",
      "[INFO] epoch=3600, loss=0.0023398\n",
      "[INFO] epoch=3700, loss=0.0022525\n",
      "[INFO] epoch=3800, loss=0.0021711\n",
      "[INFO] epoch=3900, loss=0.0020953\n",
      "[INFO] epoch=4000, loss=0.0020244\n",
      "[INFO] epoch=4100, loss=0.0019580\n",
      "[INFO] epoch=4200, loss=0.0018956\n",
      "[INFO] epoch=4300, loss=0.0018370\n",
      "[INFO] epoch=4400, loss=0.0017818\n",
      "[INFO] epoch=4500, loss=0.0017297\n",
      "[INFO] epoch=4600, loss=0.0016804\n",
      "[INFO] epoch=4700, loss=0.0016338\n",
      "[INFO] epoch=4800, loss=0.0015897\n",
      "[INFO] epoch=4900, loss=0.0015477\n",
      "[INFO] epoch=5000, loss=0.0015079\n",
      "[INFO] epoch=5100, loss=0.0014700\n",
      "[INFO] epoch=5200, loss=0.0014339\n",
      "[INFO] epoch=5300, loss=0.0013995\n",
      "[INFO] epoch=5400, loss=0.0013667\n",
      "[INFO] epoch=5500, loss=0.0013353\n",
      "[INFO] epoch=5600, loss=0.0013053\n",
      "[INFO] epoch=5700, loss=0.0012766\n",
      "[INFO] epoch=5800, loss=0.0012490\n",
      "[INFO] epoch=5900, loss=0.0012226\n",
      "[INFO] epoch=6000, loss=0.0011973\n",
      "[INFO] epoch=6100, loss=0.0011729\n",
      "[INFO] epoch=6200, loss=0.0011495\n",
      "[INFO] epoch=6300, loss=0.0011270\n",
      "[INFO] epoch=6400, loss=0.0011054\n",
      "[INFO] epoch=6500, loss=0.0010845\n",
      "[INFO] epoch=6600, loss=0.0010644\n",
      "[INFO] epoch=6700, loss=0.0010450\n",
      "[INFO] epoch=6800, loss=0.0010262\n",
      "[INFO] epoch=6900, loss=0.0010081\n",
      "[INFO] epoch=7000, loss=0.0009907\n",
      "[INFO] epoch=7100, loss=0.0009738\n",
      "[INFO] epoch=7200, loss=0.0009574\n",
      "[INFO] epoch=7300, loss=0.0009416\n",
      "[INFO] epoch=7400, loss=0.0009263\n",
      "[INFO] epoch=7500, loss=0.0009114\n",
      "[INFO] epoch=7600, loss=0.0008970\n",
      "[INFO] epoch=7700, loss=0.0008831\n",
      "[INFO] epoch=7800, loss=0.0008696\n",
      "[INFO] epoch=7900, loss=0.0008564\n",
      "[INFO] epoch=8000, loss=0.0008437\n",
      "[INFO] epoch=8100, loss=0.0008313\n",
      "[INFO] epoch=8200, loss=0.0008192\n",
      "[INFO] epoch=8300, loss=0.0008075\n",
      "[INFO] epoch=8400, loss=0.0007961\n",
      "[INFO] epoch=8500, loss=0.0007851\n",
      "[INFO] epoch=8600, loss=0.0007743\n",
      "[INFO] epoch=8700, loss=0.0007638\n",
      "[INFO] epoch=8800, loss=0.0007536\n",
      "[INFO] epoch=8900, loss=0.0007436\n",
      "[INFO] epoch=9000, loss=0.0007339\n",
      "[INFO] epoch=9100, loss=0.0007244\n",
      "[INFO] epoch=9200, loss=0.0007152\n",
      "[INFO] epoch=9300, loss=0.0007062\n",
      "[INFO] epoch=9400, loss=0.0006974\n",
      "[INFO] epoch=9500, loss=0.0006889\n",
      "[INFO] epoch=9600, loss=0.0006805\n",
      "[INFO] epoch=9700, loss=0.0006723\n",
      "[INFO] epoch=9800, loss=0.0006644\n",
      "[INFO] epoch=9900, loss=0.0006566\n",
      "[INFO] epoch=10000, loss=0.0006489\n",
      "[INFO] epoch=10100, loss=0.0006415\n",
      "[INFO] epoch=10200, loss=0.0006342\n",
      "[INFO] epoch=10300, loss=0.0006271\n",
      "[INFO] epoch=10400, loss=0.0006201\n",
      "[INFO] epoch=10500, loss=0.0006133\n",
      "[INFO] epoch=10600, loss=0.0006066\n",
      "[INFO] epoch=10700, loss=0.0006001\n",
      "[INFO] epoch=10800, loss=0.0005937\n",
      "[INFO] epoch=10900, loss=0.0005874\n",
      "[INFO] epoch=11000, loss=0.0005813\n",
      "[INFO] epoch=11100, loss=0.0005752\n",
      "[INFO] epoch=11200, loss=0.0005693\n",
      "[INFO] epoch=11300, loss=0.0005636\n",
      "[INFO] epoch=11400, loss=0.0005579\n",
      "[INFO] epoch=11500, loss=0.0005523\n",
      "[INFO] epoch=11600, loss=0.0005469\n",
      "[INFO] epoch=11700, loss=0.0005416\n",
      "[INFO] epoch=11800, loss=0.0005363\n",
      "[INFO] epoch=11900, loss=0.0005312\n",
      "[INFO] epoch=12000, loss=0.0005261\n",
      "[INFO] epoch=12100, loss=0.0005212\n",
      "[INFO] epoch=12200, loss=0.0005163\n",
      "[INFO] epoch=12300, loss=0.0005115\n",
      "[INFO] epoch=12400, loss=0.0005068\n",
      "[INFO] epoch=12500, loss=0.0005022\n",
      "[INFO] epoch=12600, loss=0.0004977\n",
      "[INFO] epoch=12700, loss=0.0004932\n",
      "[INFO] epoch=12800, loss=0.0004889\n",
      "[INFO] epoch=12900, loss=0.0004846\n",
      "[INFO] epoch=13000, loss=0.0004804\n",
      "[INFO] epoch=13100, loss=0.0004762\n",
      "[INFO] epoch=13200, loss=0.0004721\n",
      "[INFO] epoch=13300, loss=0.0004681\n",
      "[INFO] epoch=13400, loss=0.0004642\n",
      "[INFO] epoch=13500, loss=0.0004603\n",
      "[INFO] epoch=13600, loss=0.0004565\n",
      "[INFO] epoch=13700, loss=0.0004527\n",
      "[INFO] epoch=13800, loss=0.0004490\n",
      "[INFO] epoch=13900, loss=0.0004454\n",
      "[INFO] epoch=14000, loss=0.0004418\n",
      "[INFO] epoch=14100, loss=0.0004382\n",
      "[INFO] epoch=14200, loss=0.0004348\n",
      "[INFO] epoch=14300, loss=0.0004314\n",
      "[INFO] epoch=14400, loss=0.0004280\n",
      "[INFO] epoch=14500, loss=0.0004247\n",
      "[INFO] epoch=14600, loss=0.0004214\n",
      "[INFO] epoch=14700, loss=0.0004182\n",
      "[INFO] epoch=14800, loss=0.0004150\n",
      "[INFO] epoch=14900, loss=0.0004119\n",
      "[INFO] epoch=15000, loss=0.0004088\n",
      "[INFO] epoch=15100, loss=0.0004058\n",
      "[INFO] epoch=15200, loss=0.0004028\n",
      "[INFO] epoch=15300, loss=0.0003999\n",
      "[INFO] epoch=15400, loss=0.0003970\n",
      "[INFO] epoch=15500, loss=0.0003941\n",
      "[INFO] epoch=15600, loss=0.0003913\n",
      "[INFO] epoch=15700, loss=0.0003885\n",
      "[INFO] epoch=15800, loss=0.0003857\n",
      "[INFO] epoch=15900, loss=0.0003830\n",
      "[INFO] epoch=16000, loss=0.0003804\n",
      "[INFO] epoch=16100, loss=0.0003777\n",
      "[INFO] epoch=16200, loss=0.0003751\n",
      "[INFO] epoch=16300, loss=0.0003726\n",
      "[INFO] epoch=16400, loss=0.0003700\n",
      "[INFO] epoch=16500, loss=0.0003675\n",
      "[INFO] epoch=16600, loss=0.0003651\n",
      "[INFO] epoch=16700, loss=0.0003627\n",
      "[INFO] epoch=16800, loss=0.0003603\n",
      "[INFO] epoch=16900, loss=0.0003579\n",
      "[INFO] epoch=17000, loss=0.0003555\n",
      "[INFO] epoch=17100, loss=0.0003532\n",
      "[INFO] epoch=17200, loss=0.0003510\n",
      "[INFO] epoch=17300, loss=0.0003487\n",
      "[INFO] epoch=17400, loss=0.0003465\n",
      "[INFO] epoch=17500, loss=0.0003443\n",
      "[INFO] epoch=17600, loss=0.0003421\n",
      "[INFO] epoch=17700, loss=0.0003400\n",
      "[INFO] epoch=17800, loss=0.0003379\n",
      "[INFO] epoch=17900, loss=0.0003358\n",
      "[INFO] epoch=18000, loss=0.0003337\n",
      "[INFO] epoch=18100, loss=0.0003317\n",
      "[INFO] epoch=18200, loss=0.0003297\n",
      "[INFO] epoch=18300, loss=0.0003277\n",
      "[INFO] epoch=18400, loss=0.0003257\n",
      "[INFO] epoch=18500, loss=0.0003237\n",
      "[INFO] epoch=18600, loss=0.0003218\n",
      "[INFO] epoch=18700, loss=0.0003199\n",
      "[INFO] epoch=18800, loss=0.0003180\n",
      "[INFO] epoch=18900, loss=0.0003162\n",
      "[INFO] epoch=19000, loss=0.0003144\n",
      "[INFO] epoch=19100, loss=0.0003125\n",
      "[INFO] epoch=19200, loss=0.0003107\n",
      "[INFO] epoch=19300, loss=0.0003090\n",
      "[INFO] epoch=19400, loss=0.0003072\n",
      "[INFO] epoch=19500, loss=0.0003055\n",
      "[INFO] epoch=19600, loss=0.0003038\n",
      "[INFO] epoch=19700, loss=0.0003021\n",
      "[INFO] epoch=19800, loss=0.0003004\n",
      "[INFO] epoch=19900, loss=0.0002987\n",
      "[INFO] epoch=20000, loss=0.0002971\n",
      "[INFO] epoch=20100, loss=0.0002955\n",
      "[INFO] epoch=20200, loss=0.0002938\n",
      "[INFO] epoch=20300, loss=0.0002923\n",
      "[INFO] epoch=20400, loss=0.0002907\n",
      "[INFO] epoch=20500, loss=0.0002891\n",
      "[INFO] epoch=20600, loss=0.0002876\n",
      "[INFO] epoch=20700, loss=0.0002861\n",
      "[INFO] epoch=20800, loss=0.0002845\n",
      "[INFO] epoch=20900, loss=0.0002831\n",
      "[INFO] epoch=21000, loss=0.0002816\n",
      "[INFO] epoch=21100, loss=0.0002801\n",
      "[INFO] epoch=21200, loss=0.0002787\n",
      "[INFO] epoch=21300, loss=0.0002772\n",
      "[INFO] epoch=21400, loss=0.0002758\n",
      "[INFO] epoch=21500, loss=0.0002744\n",
      "[INFO] epoch=21600, loss=0.0002730\n",
      "[INFO] epoch=21700, loss=0.0002716\n",
      "[INFO] epoch=21800, loss=0.0002703\n",
      "[INFO] epoch=21900, loss=0.0002689\n",
      "[INFO] epoch=22000, loss=0.0002676\n",
      "[INFO] epoch=22100, loss=0.0002663\n",
      "[INFO] epoch=22200, loss=0.0002649\n",
      "[INFO] epoch=22300, loss=0.0002636\n",
      "[INFO] epoch=22400, loss=0.0002624\n",
      "[INFO] epoch=22500, loss=0.0002611\n",
      "[INFO] epoch=22600, loss=0.0002598\n",
      "[INFO] epoch=22700, loss=0.0002586\n",
      "[INFO] epoch=22800, loss=0.0002573\n",
      "[INFO] epoch=22900, loss=0.0002561\n",
      "[INFO] epoch=23000, loss=0.0002549\n",
      "[INFO] epoch=23100, loss=0.0002537\n",
      "[INFO] epoch=23200, loss=0.0002525\n",
      "[INFO] epoch=23300, loss=0.0002513\n",
      "[INFO] epoch=23400, loss=0.0002501\n",
      "[INFO] epoch=23500, loss=0.0002490\n",
      "[INFO] epoch=23600, loss=0.0002478\n",
      "[INFO] epoch=23700, loss=0.0002467\n",
      "[INFO] epoch=23800, loss=0.0002455\n",
      "[INFO] epoch=23900, loss=0.0002444\n",
      "[INFO] epoch=24000, loss=0.0002433\n",
      "[INFO] epoch=24100, loss=0.0002422\n",
      "[INFO] epoch=24200, loss=0.0002411\n",
      "[INFO] epoch=24300, loss=0.0002400\n",
      "[INFO] epoch=24400, loss=0.0002390\n",
      "[INFO] epoch=24500, loss=0.0002379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=24600, loss=0.0002369\n",
      "[INFO] epoch=24700, loss=0.0002358\n",
      "[INFO] epoch=24800, loss=0.0002348\n",
      "[INFO] epoch=24900, loss=0.0002338\n",
      "[INFO] epoch=25000, loss=0.0002327\n",
      "[INFO] epoch=25100, loss=0.0002317\n",
      "[INFO] epoch=25200, loss=0.0002307\n",
      "[INFO] epoch=25300, loss=0.0002297\n",
      "[INFO] epoch=25400, loss=0.0002288\n",
      "[INFO] epoch=25500, loss=0.0002278\n",
      "[INFO] epoch=25600, loss=0.0002268\n",
      "[INFO] epoch=25700, loss=0.0002259\n",
      "[INFO] epoch=25800, loss=0.0002249\n",
      "[INFO] epoch=25900, loss=0.0002240\n",
      "[INFO] epoch=26000, loss=0.0002230\n",
      "[INFO] epoch=26100, loss=0.0002221\n",
      "[INFO] epoch=26200, loss=0.0002212\n",
      "[INFO] epoch=26300, loss=0.0002203\n",
      "[INFO] epoch=26400, loss=0.0002194\n",
      "[INFO] epoch=26500, loss=0.0002185\n",
      "[INFO] epoch=26600, loss=0.0002176\n",
      "[INFO] epoch=26700, loss=0.0002167\n",
      "[INFO] epoch=26800, loss=0.0002158\n",
      "[INFO] epoch=26900, loss=0.0002149\n",
      "[INFO] epoch=27000, loss=0.0002141\n",
      "[INFO] epoch=27100, loss=0.0002132\n",
      "[INFO] epoch=27200, loss=0.0002124\n",
      "[INFO] epoch=27300, loss=0.0002115\n",
      "[INFO] epoch=27400, loss=0.0002107\n",
      "[INFO] epoch=27500, loss=0.0002099\n",
      "[INFO] epoch=27600, loss=0.0002090\n",
      "[INFO] epoch=27700, loss=0.0002082\n",
      "[INFO] epoch=27800, loss=0.0002074\n",
      "[INFO] epoch=27900, loss=0.0002066\n",
      "[INFO] epoch=28000, loss=0.0002058\n",
      "[INFO] epoch=28100, loss=0.0002050\n",
      "[INFO] epoch=28200, loss=0.0002042\n",
      "[INFO] epoch=28300, loss=0.0002034\n",
      "[INFO] epoch=28400, loss=0.0002027\n",
      "[INFO] epoch=28500, loss=0.0002019\n",
      "[INFO] epoch=28600, loss=0.0002011\n",
      "[INFO] epoch=28700, loss=0.0002004\n",
      "[INFO] epoch=28800, loss=0.0001996\n",
      "[INFO] epoch=28900, loss=0.0001989\n",
      "[INFO] epoch=29000, loss=0.0001981\n",
      "[INFO] epoch=29100, loss=0.0001974\n",
      "[INFO] epoch=29200, loss=0.0001967\n",
      "[INFO] epoch=29300, loss=0.0001959\n",
      "[INFO] epoch=29400, loss=0.0001952\n",
      "[INFO] epoch=29500, loss=0.0001945\n",
      "[INFO] epoch=29600, loss=0.0001938\n",
      "[INFO] epoch=29700, loss=0.0001931\n",
      "[INFO] epoch=29800, loss=0.0001924\n",
      "[INFO] epoch=29900, loss=0.0001917\n",
      "[INFO] epoch=30000, loss=0.0001910\n",
      "[INFO] epoch=30100, loss=0.0001903\n",
      "[INFO] epoch=30200, loss=0.0001896\n",
      "[INFO] epoch=30300, loss=0.0001890\n",
      "[INFO] epoch=30400, loss=0.0001883\n",
      "[INFO] epoch=30500, loss=0.0001876\n",
      "[INFO] epoch=30600, loss=0.0001870\n",
      "[INFO] epoch=30700, loss=0.0001863\n",
      "[INFO] epoch=30800, loss=0.0001857\n",
      "[INFO] epoch=30900, loss=0.0001850\n",
      "[INFO] epoch=31000, loss=0.0001844\n",
      "[INFO] epoch=31100, loss=0.0001837\n",
      "[INFO] epoch=31200, loss=0.0001831\n",
      "[INFO] epoch=31300, loss=0.0001825\n",
      "[INFO] epoch=31400, loss=0.0001818\n",
      "[INFO] epoch=31500, loss=0.0001812\n",
      "[INFO] epoch=31600, loss=0.0001806\n",
      "[INFO] epoch=31700, loss=0.0001800\n",
      "[INFO] epoch=31800, loss=0.0001794\n",
      "[INFO] epoch=31900, loss=0.0001788\n",
      "[INFO] epoch=32000, loss=0.0001782\n",
      "[INFO] epoch=32100, loss=0.0001776\n",
      "[INFO] epoch=32200, loss=0.0001770\n",
      "[INFO] epoch=32300, loss=0.0001764\n",
      "[INFO] epoch=32400, loss=0.0001758\n",
      "[INFO] epoch=32500, loss=0.0001752\n",
      "[INFO] epoch=32600, loss=0.0001746\n",
      "[INFO] epoch=32700, loss=0.0001741\n",
      "[INFO] epoch=32800, loss=0.0001735\n",
      "[INFO] epoch=32900, loss=0.0001729\n",
      "[INFO] epoch=33000, loss=0.0001724\n",
      "[INFO] epoch=33100, loss=0.0001718\n",
      "[INFO] epoch=33200, loss=0.0001712\n",
      "[INFO] epoch=33300, loss=0.0001707\n",
      "[INFO] epoch=33400, loss=0.0001701\n",
      "[INFO] epoch=33500, loss=0.0001696\n",
      "[INFO] epoch=33600, loss=0.0001690\n",
      "[INFO] epoch=33700, loss=0.0001685\n",
      "[INFO] epoch=33800, loss=0.0001680\n",
      "[INFO] epoch=33900, loss=0.0001674\n",
      "[INFO] epoch=34000, loss=0.0001669\n",
      "[INFO] epoch=34100, loss=0.0001664\n",
      "[INFO] epoch=34200, loss=0.0001659\n",
      "[INFO] epoch=34300, loss=0.0001653\n",
      "[INFO] epoch=34400, loss=0.0001648\n",
      "[INFO] epoch=34500, loss=0.0001643\n",
      "[INFO] epoch=34600, loss=0.0001638\n",
      "[INFO] epoch=34700, loss=0.0001633\n",
      "[INFO] epoch=34800, loss=0.0001628\n",
      "[INFO] epoch=34900, loss=0.0001623\n",
      "[INFO] epoch=35000, loss=0.0001618\n",
      "[INFO] epoch=35100, loss=0.0001613\n",
      "[INFO] epoch=35200, loss=0.0001608\n",
      "[INFO] epoch=35300, loss=0.0001603\n",
      "[INFO] epoch=35400, loss=0.0001598\n",
      "[INFO] epoch=35500, loss=0.0001593\n",
      "[INFO] epoch=35600, loss=0.0001589\n",
      "[INFO] epoch=35700, loss=0.0001584\n",
      "[INFO] epoch=35800, loss=0.0001579\n",
      "[INFO] epoch=35900, loss=0.0001574\n",
      "[INFO] epoch=36000, loss=0.0001570\n",
      "[INFO] epoch=36100, loss=0.0001565\n",
      "[INFO] epoch=36200, loss=0.0001560\n",
      "[INFO] epoch=36300, loss=0.0001556\n",
      "[INFO] epoch=36400, loss=0.0001551\n",
      "[INFO] epoch=36500, loss=0.0001547\n",
      "[INFO] epoch=36600, loss=0.0001542\n",
      "[INFO] epoch=36700, loss=0.0001538\n",
      "[INFO] epoch=36800, loss=0.0001533\n",
      "[INFO] epoch=36900, loss=0.0001529\n",
      "[INFO] epoch=37000, loss=0.0001524\n",
      "[INFO] epoch=37100, loss=0.0001520\n",
      "[INFO] epoch=37200, loss=0.0001515\n",
      "[INFO] epoch=37300, loss=0.0001511\n",
      "[INFO] epoch=37400, loss=0.0001507\n",
      "[INFO] epoch=37500, loss=0.0001502\n",
      "[INFO] epoch=37600, loss=0.0001498\n",
      "[INFO] epoch=37700, loss=0.0001494\n",
      "[INFO] epoch=37800, loss=0.0001490\n",
      "[INFO] epoch=37900, loss=0.0001485\n",
      "[INFO] epoch=38000, loss=0.0001481\n",
      "[INFO] epoch=38100, loss=0.0001477\n",
      "[INFO] epoch=38200, loss=0.0001473\n",
      "[INFO] epoch=38300, loss=0.0001469\n",
      "[INFO] epoch=38400, loss=0.0001465\n",
      "[INFO] epoch=38500, loss=0.0001461\n",
      "[INFO] epoch=38600, loss=0.0001457\n",
      "[INFO] epoch=38700, loss=0.0001453\n",
      "[INFO] epoch=38800, loss=0.0001448\n",
      "[INFO] epoch=38900, loss=0.0001445\n",
      "[INFO] epoch=39000, loss=0.0001441\n",
      "[INFO] epoch=39100, loss=0.0001437\n",
      "[INFO] epoch=39200, loss=0.0001433\n",
      "[INFO] epoch=39300, loss=0.0001429\n",
      "[INFO] epoch=39400, loss=0.0001425\n",
      "[INFO] epoch=39500, loss=0.0001421\n",
      "[INFO] epoch=39600, loss=0.0001417\n",
      "[INFO] epoch=39700, loss=0.0001413\n",
      "[INFO] epoch=39800, loss=0.0001410\n",
      "[INFO] epoch=39900, loss=0.0001406\n",
      "[INFO] epoch=40000, loss=0.0001402\n",
      "[INFO] data=[0 0], expected=[0], pred=[[0.00816274]], step=0\n",
      "[INFO] data=[0 1], expected=[1], pred=[[0.9923435]], step=1\n",
      "[INFO] data=[1 0], expected=[1], pred=[[0.99241094]], step=1\n",
      "[INFO] data=[1 1], expected=[0], pred=[[0.00987715]], step=0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork([2, 2, 1], learning_rate=0.5)\n",
    "nn.fit(X, y, epochs=40000)\n",
    "\n",
    "for i in range(0, 4):\n",
    "    x = X[i]\n",
    "    pred = nn.predict(x)\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, expected={}, pred={}, step={}\".format(x, y[i], pred, step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
